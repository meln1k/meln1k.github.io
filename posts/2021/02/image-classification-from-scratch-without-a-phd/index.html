<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Image Classification from scratch without a PhD | Nikita Melkozerov</title><meta name=keywords content="fast.ai"><meta name=description content="Let's write a handwritten digit classifier from scratch, without prior ML experience"><meta name=author content><link rel=canonical href=https://nikita.melkozerov.dev/posts/2021/02/image-classification-from-scratch-without-a-phd/><link href=/assets/css/stylesheet.min.1eef9c740af75b4e5f773d9d5f757e03e3df71f3a308e73070cc73d55a59a7d7.css integrity="sha256-Hu+cdAr3W05fdz2dX3V+A+PfcfOjCOcwcMxz1VpZp9c=" rel="preload stylesheet" as=style><link rel=icon href=https://nikita.melkozerov.dev/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://nikita.melkozerov.dev/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://nikita.melkozerov.dev/favicon-32x32.png><link rel=apple-touch-icon href=https://nikita.melkozerov.dev/apple-touch-icon.png><link rel=mask-icon href=https://nikita.melkozerov.dev/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.81.0"><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-59947059-1','auto'),ga('send','pageview'))</script><script async src=https://www.google-analytics.com/analytics.js></script><meta property="og:title" content="Image Classification from scratch without a PhD"><meta property="og:description" content="Let's write a handwritten digit classifier from scratch, without prior ML experience"><meta property="og:type" content="article"><meta property="og:url" content="https://nikita.melkozerov.dev/posts/2021/02/image-classification-from-scratch-without-a-phd/"><meta property="article:published_time" content="2021-02-24T22:47:11+01:00"><meta property="article:modified_time" content="2021-02-24T22:47:11+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Image Classification from scratch without a PhD"><meta name=twitter:description content="Let's write a handwritten digit classifier from scratch, without prior ML experience"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://nikita.melkozerov.dev/posts/"},{"@type":"ListItem","position":2,"name":"Image Classification from scratch without a PhD","item":"https://nikita.melkozerov.dev/posts/2021/02/image-classification-from-scratch-without-a-phd/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Image Classification from scratch without a PhD","name":"Image Classification from scratch without a PhD","description":"Hello folks!\nRecently I trained a handwritten digit classifier using the MNIST dataset from scratch, and it was an eye-opening experience for me. What looked like magic to me …","keywords":["fast.ai"],"articleBody":"Hello folks!\nRecently I trained a handwritten digit classifier using the MNIST dataset from scratch, and it was an eye-opening experience for me. What looked like magic to me before now is just a few mathematical concepts applied together. I was very excited about how simple the whole process was, and I want to share it with everyone who still wonders what kind of magic is happening inside.\nAll you need to know is a bit of python and a few concepts from high-school math. No previous machine learning background is necessary.\nThis post is inspired by chapter 4 of the fast.ai book (I highly recommend it if you’re getting started with deep learning), where we tried to build a classifier that can recognize 3’s and 7’s from the MNIST dataset. This time, however, we will train a classifier to recognize all ten digits of the dataset. I will also try to avoid using “magical” high-level components as much as possible.\nLet’s go!\nWe will use PyTorch and fast.ai libraries for a few useful utilities.\nFirst, we need to import a few functions from the fastai library:\nfrom fastai.vision.all import * from fastbook import * matplotlib.rc('image', cmap='Greys') Downloading the dataset Let’s download the dataset first. Fast.ai has a convenient function to quickly obtain the dataset we’re going to use for training:\npath = untar_data(URLs.MNIST) path.ls() (#2) [Path('/home/nm/.fastai/data/mnist_png/training'),Path('/home/nm/.fastai/data/mnist_png/testing')]  The dataset has two folders inside it, training and testing. training should be used for the model training and validation, and testing used to compare the accuracy between different models.\n A small note: I will do a shortcut here and use the testing dataset to validate my model. That is something you shouldn’t do in production when you want to evaluate the model’s performance. Instead, you should split the training set into training/validation parts. For our toy problem, however, and for the sake of simplicity, I will use the testing set for validation.\n Let’s peek inside the training folder:\n(path/'training').ls() (#10) [ Path('/home/nm/.fastai/data/mnist_png/training/8'), Path('/home/nm/.fastai/data/mnist_png/training/3'), Path('/home/nm/.fastai/data/mnist_png/training/7'), Path('/home/nm/.fastai/data/mnist_png/training/5'), Path('/home/nm/.fastai/data/mnist_png/training/9'), Path('/home/nm/.fastai/data/mnist_png/training/4'), Path('/home/nm/.fastai/data/mnist_png/training/2'), Path('/home/nm/.fastai/data/mnist_png/training/6'), Path('/home/nm/.fastai/data/mnist_png/training/1'), Path('/home/nm/.fastai/data/mnist_png/training/0') ]  What we have here is ten folders, one for each digit. Every folder contains thousands of images of size 28x28 representing some digit.\nYou might say: “Well, that’s just a bunch of images compressed with a PNG algorithm. How can we do any math operations on them?”. You’re correct, plain image files are not very useful. To get started, we will first transform the images into a tensor.\n“Wait, what is a tensor?” you ask, and I’d say it a fancy name for a multi-dimensional array. The number of dimensions of this array is also called a rank of a tensor.\nFor example:\nA zero-dimensional array (it is also called scalar) 1 is a tensor of rank 0.\n1-d array aka vector [1,2,3] is a tensor of rank 1.\n2-d arrray aka matrix\n[[1,2,3], [4,5,6], [7,8,9]] is a tensor of rank 2, and so on.\nWe want to convert the images to tensors because then we can perform math operations on them.\nNow we have an image, which is a 2-d array of pixels with pixel intensity values ranging from 0 to 255. Next, we will represent it as a tensor of a rank 2, and scale the pixel values between 0 and 1. Scaling the pixels between 0 and 1 will give us a convenient abstraction for some of the actions we will do next.\nOnce we created a tensor from one image, we will convert other images in our dataset to tensors as well. Then, we will stack the image tensors together into a single tensor of rank 3. I’ll explain why we do this in a bit.\nLet’s write a function for this:\ndef image_path_to_tensor(path): images_paths = path.ls() tensors = [tensor(Image.open(p)) for p in images_paths] return torch.stack(tensors).float()/255 Having defined the function, let’s use it to transform all the images in 10 folders that we have:\nstacked_tensors = [image_path_to_tensor(image_folder) for image_folder in (path/'training').ls().sorted()] The result is ten tensors that represent the training set of each digit. Let’s look at one digit and check the shape of the tensor:\nstacked_tensors[4].shape torch.Size([5842, 28, 28])  The first dimension of the tensor corresponds to an image file, second and third dimensions are the height and the width of the image in pixels.\nBecause we had 5824 images for digit 4, and every image has the size of 28x28 pixels, therefore the resulting shape of the tensor is 5842 by 28 by 28. That’s a tensor of rank 3.\nBuilding a baseline: pixel difference Before building any machine learning model it is generally a good idea to come up with a simple baseline first. One obvious choice would be a random baseline, where each digit is chosen at random, but let’s try a little bit harder and build a baseline that relies on pixel similarity.\nThe idea is next: we will average the image tensors along the first axis, and it will be our “pretty average digit”. Then we will compare how similar each pixel of the image we try to classify to each pixel of every “average” digit, and our final guess will be the digit with the smallest error.\nTo start, let’s compute our “average” digit images:\nimage_means = [None] * 10 for digit, imgs_tensor in enumerate(stacked_tensors): mean_tensor = imgs_tensor.mean(0) image_means[digit] = mean_tensor For example, this is how an “average” 4 looks like:\nNow we need a function that can compute the error. One possible option is to subtract the pixels of two images, square them to make them between 0 and 1, and then take the mean of the result:\ndef calculate_error(image, label): ideal_tensor = image_means[label] return ((image - ideal_tensor)**2).mean() Let’s try it out on a digit 3:\nThe average digit of 3 looks like this:\ncalculate_error(stacked_tensors[3][0], 3) tensor(0.0632)  This function is also called mean squared error.\nNow let’s create a validation set. As I already mentioned above, using the testing set as a validation set is a shortcut for simplicity, and you should not do the same in production.\nvalid_stacked = [image_path_to_tensor(image_folder) for image_folder in (path/'testing').ls().sorted()] We’re able to compute an error for a single image. Our job now is to compute the error for all images in the validation set. Unfortunately, we can’t simply write a loop and do this.\nThe main issue is python loops are unlikely to be vectorized on the GPU. If we take a single image, put it on the GPU, calculate the error and then remove it from the GPU, it will take us quite a while to calculate the error for the whole validation set.\nInstead of using a loop, we will grab the tensor with images, put it on the GPU, and then calculate the error for all images in parallel. GPUs are super fast number-crunching machines, and it will take no time for our GPU to do that.\nBut how can we do this? Welcome to broadcasting. Broadcasting is a technique that allows PyTorch to perform operations on tensors with different shapes in a way as they were same-shape tensors. Let me explain it with a simple example.\nIf we have two tensors:\na = [[1,2], [3,4], [5,6]] and\nb = [1,1] then if we write something like c = a + b, we will get the next tensor as a result:\nc = [[2,3], [4,5], [6,7]] Tensor b was added to each element of the tensor a even though they have different ranks. Simply speaking, instead of looping through all elements of the vector a, we told PyTorch to do this in a declarative fashion.\nWith broadcasting in mind, let’s define a few functions to measure the distances between our images.\nNaïve approach with subtracting one image pixes from another and taking the mean will not work very well. It is possible to have drastically different images where dark and bright pixels will compensate each other, and the mean will be close to zero. To solve this problem, we should make the difference positive and then take the mean.\nFor example, we can calculate a mean absolute error (MAE) and a mean squared error (MSE). The difference between them is that MSE would result in a higher error when images are very different.\ndef mnist_distance_mae(a,b): return (a-b).abs().mean((-1,-2)) def mnist_distance_mse(a,b): return torch.square((a-b)).mean((-1,-2)) The functions above subtract pixels of two images, then take the absolute/squared value and computing the mean. (-1,-2) means that we want to compute the mean along the last and last-1 axis of our tensor, which corresponds to 28x28 image.\n When I was implementing this code, broadcasting led to some very nasty bugs. For example, I had a situation where my model was not able to make any progress in training. Another time the training was much slower than I expected, and I did not know why. As it turned out, there was a bug in the loss function because of implicit broadcasting, and it took me a lot of time to localize the problem. It is very important to check that the shape of the returned tensor on every stage corresponds to what we expect.\n Let’s see what is the shape of the result of our distance function:\nmnist_distance_mae(valid_stacked[3], image_means[3]).shape torch.Size([1010])  It is a tensor of size 1010, with one distance per one validation image. That seems to be correct, and we can continue.\nmnist_distance_mse(valid_stacked[3], image_means[3]) tensor([0.0575, 0.0516, 0.0542, ..., 0.0480, 0.0642, 0.0429])  Given the distance functions, now we can write a function which will tell us if our digit prediction is correct:\ndef analyze_digit(digit: str, candidate, distance_function): distances = torch.stack([distance_function(candidate, mean_tensor) for mean_tensor in image_means]) lowest_distance = torch.argmin(distances, dim=0) return lowest_distance == digit analyze_digit(4, stacked_tensors[3][113], mnist_distance_mae) tensor(False)  Let’s put broadcasting into work and do analyze all 3s at once:\nanalyze_digit(3, valid_stacked[3], mnist_distance_mae).float().mean() tensor(0.6089)  Our accuracy is about 60%. It is better than random (it would be around 10), but we can do better. What is the mean accuracy for all digits that we have? We can use both of our distance functions and compare the results:\naccuracy_mae = torch.stack([analyze_digit(digit, valid_stacked[digit], mnist_distance_mae).float().mean() for digit in range(10)]) accuracy_mae tensor([0.8153, 0.9982, 0.4234, 0.6089, 0.6680, 0.3262, 0.7871, 0.7646, 0.4425, 0.7760])  accuracy_mse = torch.stack([analyze_digit(digit, valid_stacked[digit], mnist_distance_mse).float().mean() for digit in range(10)]) accuracy_mse tensor([0.8959, 0.9621, 0.7568, 0.8059, 0.8259, 0.6861, 0.8633, 0.8327, 0.7372, 0.8067])  As we see, MSE gives us better results since it penalizes bigger differences more compared to smaller ones. Let’s look at the mean accuracies:\naccuracy_mae.mean() tensor(0.6610)  accuracy_mse.mean() tensor(0.8173)  One observation is that MSE gives us better accuracy compared to MAE.\nWe got 82% accuracy without any learning at all! We can take it as our baseline, and our goal would be to train a machine learning model which can do this better. It is always a good idea to start with a simple solution and then move to a more sophisticated one if the observed performance is below expectations.\nMachile Learning time! Now, we’re finally going to do some machine learning.\nDatasets First, we will concatenate our images for different digits into a single tensor, and then we will represent every 28x28 image as 1x784 vector. I’ll explain why we do it in a bit. This tensor is going to be the “input” for our model.\ntrain_x = torch.cat([stacked_tensors[i] for i in range(10)]).view(-1, 28*28) Next, let’s create another tensor containing the correct labels for every image in our previous tensor.\ntrain_y = torch.cat([torch.stack([tensor(i)]*len(stacked_tensors[i])) for i in range(10)]) train_x.shape,train_y.shape (torch.Size([60000, 784]), torch.Size([60000]))  Finally, we will combine them in a dataset. A dataset is just a simple pair of inputs and outputs to the model, in our case, it is the images (train_x) and the labels (train_y).\ndataset = list(zip(train_x,train_y)) We can also look at what is inside. To do this, we want to convert 1x784 tensor into a 28x28 one. Method view of a tensor can help us with it:\nshow_image(dataset[32000][0].view(-1,28,28)) And the label is also 5:\ndataset[32000][1] tensor(5)  Now we perform the same operations for the validation set:\nvalid_x = torch.cat([valid_stacked[i] for i in range(10)]).view(-1, 28*28) valid_y = torch.cat([torch.stack([tensor(i)]*len(valid_stacked[i])) for i in range(10)]) valid_x.shape,valid_y.shape (torch.Size([10000, 784]), torch.Size([10000]))  valid_dset = list(zip(valid_x,valid_y)) Linear model Before we continue, let’s quickly talk about what we have done and what we’re going to do next.\nWe have already collected the training and the validation datasets, and the dataset is just a collection of images and the corresponding labels. We also transformed every image into a vector.\nWhat we need now is to define a function, which will take an image as an input and will produce the prediction as an output.\nTo be more specific, our function will accept a vector of size 784 as an input, where each element of this vector will correspond to a pixel of an image. Since our function does not care about the arrangement of input values, we reshaped our tensor earlier to make it more convenient for us to work with it. The produced result will be a vector of size 10, where each number will represent the probability of each digit.\nIn the code, it would be something like this:\ndef predict(pixels: List[float]): List[float] But what kind of function is capable of doing such a transformation? To our luck, such functions exist, and they are called neural networks!\nIt is proven that given enough parameters, a neural network with only one hidden layer is capable of approximating any function with an arbitrary level of precision. How cool is that! For more information, look for the universal approximation theorem.\nWe, however, will start with a degenerate case of a neural network called a linear function.\nIn pseudocode it looks like this:\ndef linear(x: float, weight: float, bias: float) - float: return x * weight + bias This function accepts an input, number x, a parameter called weight, and a parameter called bias. Then the input is multiplied by the weight and the bias added in the end. Sounds simple, right?\nHowever, in our case, we have not a single but 784 input parameters. Let’s for now pretend that we’re only interested in predicting a single digit:\nfrom typing import List def linear(x: List[float], weights: List[float], bias: float) - float: return (x * weights).sum() + bias Here * stand for element-wise vector multiplication, meaning that if we have two vectors,\na = [1, 2, 3] b = [1, 2, 3] then a * b would be the vector [1*1, 2*2, 3*3]\nThe result of this function will be a likeliness that our input x corresponds to a label. But what if we want to predict more than one label?\nThat’s simple as well. First, we define a few sets of weights and biases, one per class we want to predict. Then we call the function several times to make a prediction for each class. The number returned by the functions represents neural net confidence in the predicted digit. Then we interpret the result with the highest number as the predicted class. Let’s say we only want to predict three digits, then what we want to do is next:\nscore_0 = linear(image, weights_0, bias_0) score_1 = linear(image, weights_1, bias_1) score_2 = linear(image, weights_2, bias_2) Finally, we check which resulted in the highest score, and this is going to be the predicted class.\nLet’s wrap it in a function:\ndef predict(image: List[float], weights: List[List[float]], biases: List[float]) - List[float]: results = [] for digit in range(3): likeliness = linear(image, weights[digit], biases[digit]) results.append(likeliness) return results That’s it! Our simple linear model will be able to predict an image from pixels.\nHowever, we have two problems:\n This function is going to be extremely slow since GPUs do not like loops, and the loop we defined earlier will be executed using plain python runtime. We don’t know where to get these magical weights and biases!  To deal with the first problem, we will utilize PyTorch tensors, which will give us free GPU parallelization.\nAs for the second problem, we need to learn about Stochastic Gradient Descent (SGD).\nEverything is a Tensor Since we want to use GPU for computation, we need to represent our weights and biases as tensors. Let’s write a helper function for this.\ndef init_params(size): return torch.randn(size).requires_grad_() Method .requires_grad_() tells PyTorch to track the operations done with the tensor so that later we can ask PyTorch to compute gradients for us. More on that in a bit.\nRemember the weights in the predict function? We wanted to have something like this List[List[float; 784]; 10] (The number next to the type is the size of the list). That is nothing else than a matrix of size 784 by 10.\nIf you’re wondering where we got these magic numbers, it comes from the following. The number 784 used because our input image has a size of 28x28, and later we unroll it into a 1x784 vector. Number 10 comes from the ten classes of digits we’re going to recognize.\nLet’s initialize the weights randomly:\nweights = init_params((28*28,10)) The same goes for biases:\nbiases = init_params(10) Now we can try to calculate the predictions for a single image. Instead of looping for every digit and multiplying vectors, we can do everything in one single step using matrix-vector multiplication. And PyTorch will use the power of the GPU to do this operation as quickly as possible!\ntrain_x[34000] @ weights + biases tensor([ 5.4221, -1.5908, 17.0564, -4.3119, -15.0097, -8.3001, 0.5106, 2.3884, -6.6653, 13.2021], grad_fn=)  The new operator @ above is a PyTorch operator for matrix multiplication. I will not get into details about what matrix multiplication is, but a nice and intuitive explanation of it can be found on http://matrixmultiplication.xyz/.\nLet’s write a function that will represent our linear model\ndef linear_model(x_batch): return x_batch @ weights + biases predictions = linear_model(train_x[34000]) predictions tensor([ 5.4221, -1.5908, 17.0564, -4.3119, -15.0097, -8.3001, 0.5106, 2.3884, -6.6653, 13.2021], grad_fn=)  One interesting property of our matrix-powered function is that this function can process a batch of several images at once. Instead of providing a single vector as an input, we can provide a matrix containing a few images, then do the matrix multiplication, and then we will get the matrix with the predictions back.\nWith this in mind, we can calculate predictions for the whole dataset without writing a single loop:\npredictions = linear_model(train_x) predictions.shape torch.Size([60000, 10])  argmax will return the index of the tensor with the biggest value. Since we expect the model to return bigger numbers for the predictions it is more confident with, the number returned will be the predicted class. Let’s calculate the accuracy:\n(predictions.argmax(dim=1) == train_y).float().mean().item() 0.12043333053588867  The model initialized using random parameters made around 10% of predictions correct. That’s pretty much what we would expect for a random model since the probability of being correct with a random guess is 0.1.\nSGD So we solved the first problem, the prediction function is fast and can be run on a GPU. But we still have no idea where to get the proper (non-random) parameters.\nNow, let’s talk about gradient descent (the “stochastic” part will be explained later).\nAs you might remember from a calculus course, a derivative of a function is another function that represents a rate of change of a function output with respect to its input. If we evaluate a derivative at some point where our function is defined, we can think of a result as a direction in which the function evaluated at that point will grow the fastest. If we know this, we can easily figure out in which direction we need to go in order to reach the minimum of the function.\nIn case our function operates on several input variables, instead of the derivative we will calculate a gradient, which can be imagined as an “advanced” version of the derivative capable of working with multi-variable functions. The gradient is similar to the derivative since it will give us the direction in which the function is going to grow the fastest.\nOnce we have the gradient, we can multiply it by -1 to get the direction in which the function will decline the fastest, then multiply the gradient by a small value and update the parameters. After we do this, the function we’re interested in will produce a smaller value given the updated parameters.\nUsing this information, we can describe the gradient descent process as following:\ninitialize function parameter randomly while (function parameters are not good enough): calculate gradient parameters = parameters - gradient * step The gradient descent is capable of optimizing any differentiable function, and we will utilize this for our training process.\nNext, we need to define a so-called loss function, which will tell us how good or bad the performance of our linear model is when we change the parameters.\nOnce we define it, we will run the gradient descent method with the loss function and find the optimal parameters.\nLoss Function Let’s think about what we want from our loss function. We want it to take images, parameters, and labels as an input and produce a number that shows how “bad” our linear model is.\nTaking this into account, our first implementation of the loss function will look like this:\ndef mnist_loss(predictions, targets, nr_classes=10): predictions = predictions.sigmoid() mask = F.one_hot(targets, nr_classes) errors = ((mask-predictions) ** 2) return errors.mean() Let’s analyze it line by line:\npredictions = predictions.sigmoid() We apply a sigmoid function to each prediction. It looks like this:\ndef sigmoid(x): return 1/(1+torch.exp(-x)) The sigmoid function maps its input to a range between 0 and 1, and it looks like this:\nOur idea is to subtract correct predictions from 1-s, so that when our model is correct and confident in the prediction, the error will be close to zero. With other predictions, we will not do anything, which will result in a low error of the model does not give us high confidence for incorrect labels.\nmask = F.one_hot(targets, nr_classes) Here we generate a mask, which will represent the correct labels. For example F.one_hot(tensor([0,2,1]), 3) will give us\n[[1,0,0], [0,0,1], [0,1,0]] Next, we will subtract our prediction from the mask. To deal with negative numbers we will take a squared value of the error, which will keep the function differentiable and additionally will have a nice bonus of punishing big errors more than small ones.\nerrors = ((mask-predictions) ** 2) In the end, we will take a mean to reduce the error tensor to a single number:\nerrors.mean() Having defined a lost function, we want to calculate its gradient. However, we run into a problem: our loss function operates on predictions and labels and not on the model parameters.\nTo include the weights and biases into the gradient calculating, we will take a derivative of a new function which is a composition of the model and the loss function. The new function will look like this:\ndef composed(x_batch, y_batch): preds = linear_model(x_batch) loss = mnist_loss(preds, y_batch) Now we need to calculate a gradient. If we do that by hand via calculating partial derivatives using a chain rule, the process would be a bit tedious because this function operates on thousands of parameters.\nFortunately for us, PyTorch can solve this quickly. What we need to do is to call .requires_grad() on a tensor in which gradients we’re interested before computing the function, and then call .backward() to tell PyTorch that we want the gradients.\nHere is how we do it:\ndef calc_grad(x_batch, y_batch, model, loss_fn): preds = model(x_batch) loss = loss_fn(preds, y_batch) loss.backward() Let’s talk about how we will feed the data to our gradient descent process. One option is to feed it one image at a time, but this will take an unreasonable amount of time to complete due to data moving overhead. Training on the whole dataset at once is not desirable either, since the dataset might be simply too big to fit on the GPU. What is commonly done is the following: the dataset is divided in batches, and then we calculate gradients using the whole batch. That’s where the word stochastic comes from, mainly because batched contain shuffled items, and the gradient descent is run on data randomly sampled from the dataset.\nWhen we iterate through the dataset, we generally prefer to have diverse examples in our batches because this leads to better generalization. An easy way to achieve this is to shuffle the dataset. Fortunately for us, PyTorch provides a class called DataLoader, which does the shuffling and batch separation for us:\ndl = DataLoader(dataset, batch_size=256,shuffle=True) valid_dl = DataLoader(valid_dset, batch_size=256, shuffle=False) Training loop We can also write a function that runs SGD using the whole dataset (an iteration through all images called an epoch):\ndef train_epoch(model, loss_fn, lr, params): for xb,yb in dl: calc_grad(xb, yb, model, loss_fn) for p in params: p.data -= p.grad * lr p.grad.zero_() For every batch, we compute the gradients, multiply them by a small number called the learning rate, and subtract the result from our initial parameters. The learning rate should be low enough to keep the process stable and at the same time large enough so that our training does not last forever.\nIt is important to know how accurate our model is. We will start with the accuracy of a single batch:\ndef batch_accuracy(xb, yb): correct = (xb.argmax(axis=1) == yb).float().mean() return correct Once we know how to measure the accuracy of the batch, we can measure the accuracy of the model in a single epoch:\ndef validate_epoch(model): accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl] return round(torch.stack(accs).mean().item(), 4) Let’s see what the accuracy of our randomly-initialized model is:\nvalidate_epoch(linear_model) 0.1272  Something around 10% is what we expect.\nFinally, we can train a single epoch:\naccuracy = [] lr = 1. # initializing random weights weights = init_params((28*28,10)) # initializing random biases biases = init_params(10) params = weights,biases # training an epoch train_epoch(linear_model, mnist_loss, lr, params) # validating results accuracy.append(validate_epoch(linear_model)) accuracy[0] 0.1405  Let’s train for 40 epochs:\nfor i in range(40): train_epoch(linear_model, mnist_loss, lr, params) accuracy.append(validate_epoch(linear_model)) plt.figure(figsize=(10,6)) plt.plot(accuracy) plt.ylabel('accuracy') plt.xlabel('epoch') plt.show() The training starts fast, but it slows down as model accuracy increases. The problem is our model can be very certain about several different classes for the same image. However, this is not the task we’re trying to solve. The reason for this behavior lies in our loss function. Remember, the loss function first applies sigmoid to keep the function output between 0 and 1. This only scales the output, but what we want is that the model will select one label at the end.\nLet’s replace the sigmoid function with a so-called softmax function. It is similar to sigmoid when we’re predicting a single class, but it will make the output probabilities sum to 1 if we have more than one class. Thus, only the relative difference of the predictions will be important, and a high degree of confidence in one class will automatically decrease confidence in other classes.\ndef mnist_loss_softmax(predictions, targets, nr_classes=10): predictions = torch.softmax(predictions, axis=1) mask = F.one_hot(targets, nr_classes) errors = ((mask-predictions) ** 2) return errors.mean() Let’s train the model again:\naccuracy = [] lr = 1. # initializing random weights weights = init_params((28*28,10)) # initializing random biases biases = init_params(10) params = weights,biases # training an epoch train_epoch(linear_model, mnist_loss_softmax, lr, params) # validating results accuracy.append(validate_epoch(linear_model)) accuracy[0] 0.1619  for i in range(40): train_epoch(linear_model, mnist_loss_softmax, lr, params) That helped, but our learning is still getting slower as the accuracy is getting higher, and we have not yet surpassed our baseline.\nLet’s blame the loss function again! Now we can think of a problem we face as following: for our loss function the absolute difference between parameters that give 0.9 and 0.99 accuracies is small, about 0.1. But if we think about it, the second set would give us 10x more accurate results! To address this problem, we’re going to apply a negative logarithm fucntion to the results. It will rescale our outputs in a way so the SGD will find the right direction easier.\nThe negative logarithm function looks like this:\nSuch a loss function is then called a cross-entropy loss:\ndef cross_entropy_loss(preds, y): # apply softmax preds = torch.softmax(preds, axis=1) # get confidences for the correct class idx = len(preds) confidences = preds[range(idx), y] # calculate negative log likelihood and return its mean log_ll = -torch.log(confidences) return log_ll.mean() Let’s train the model again using the new shiny loss function:\naccuracy = [] lr = 1. # initializing random weights weights = init_params((28*28,10)) # initializing random biases biases = init_params(10) params = weights,biases # training an epoch train_epoch(linear_model, cross_entropy_loss, lr, params) # validating results accuracy.append(validate_epoch(linear_model)) accuracy[0] 0.8431  Wow, already after training for a single epoch we got 85% accuracy, outperforming our baseline. That’s a huge improvement! Let’s train for a few more epochs:\nfor i in range(40): train_epoch(linear_model, cross_entropy_loss, lr, params) accuracy.append(validate_epoch(linear_model)) plt.figure(figsize=(10,6)) plt.plot(accuracy) plt.ylabel('accuracy') plt.xlabel('epoch') plt.show() 91.5% accuracy with a simple linear model! Isn’t it impressive?\nBut so far out model can hardly be called a neural network since it consists of a single linear layer. To make it a “real” neural net, we should add some-non-linearity. For example, we can add a sigmoid layer in-between two linear layers and then use function composition to apply them sequentially:\nw1 = init_params((28*28,64)) b1 = init_params(64) w2 = init_params((64,10)) b2 = init_params(10) def simple_neural_net(xb): res = xb@w1 + b1 # 1st linear layer res = torch.sigmoid(res) # 2nd non-linear layer res = res@w2 + b2 # 3rd linear layer return res params = [w1,b1,w2,b2] accuracy = [] lr = 1. # training an epoch train_epoch(simple_neural_net, cross_entropy_loss, lr, params) # validating results validate_epoch(simple_neural_net) 0.8372  for i in range(40): train_epoch(simple_neural_net, cross_entropy_loss, lr, params) Around 95% accuracy! That’s definitely better compared to the plain linear model.\nAs you can see, our new non-linear model is a drop-in replacement for the old one (as long as it has the same number of inputs and outputs), and the rest of the process stays exactly the same.\nFinally, as a fun example let’s see how this task can be done using a neural network designed for image recognition:\ndls = ImageDataLoaders.from_folder(path, train=\"training\", valid=\"testing\") learn = cnn_learner(dls, resnet18, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(4, 0.1)    epoch train_loss valid_loss accuracy time     0 1.519870 1.372195 0.914800 00:33   1 0.698865 53.716496 0.968600 00:32   2 0.167927 0.049071 0.988100 00:33   3 0.031542 0.020860 0.994100 00:34    Wow, 99.4% only after four epochs! Those modern neural networks are powerful.\nIt is worth mentioning that SGD is not ideal. For example, there are no guarantees that it actually finds good parameters in a finite number of steps, let alone optimal ones.\nAlso, a neural network is a non-convex function, so there may be many local minimums where the SGD can get stuck. This is why simpler methods are often better suited to problems that can be solved with the indicated simpler methods.\nFinally, ML researchers have made significant progress in the area of SGD, and neural networks and modern SGD + regularization have achieved state-of-the-art results for many problems.\nRecap We started with a simple pixel similarity baseline, which is capable of achieving 81% accuracy without any learning.\nThen we moved on to a simple linear model, which we optimized using gradient descent. As we have seen, it is nothing more complicated than a combination of matrix multiplication and derivative calculation.\nFinally, we replaced our linear model with a neural network and saw some accuracy improvements.\nThere is something cool in combining a neural net (which is capable of approximating any function giving the right parameters), and gradient descent (which is capable of finding good parameters for any differentiable function) together, isn’t it?\nAcknowledgements A big thank you goes to Felix Patzelt for reviewing this post.\n","wordCount":"5243","inLanguage":"en","datePublished":"2021-02-24T22:47:11+01:00","dateModified":"2021-02-24T22:47:11+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://nikita.melkozerov.dev/posts/2021/02/image-classification-from-scratch-without-a-phd/"},"publisher":{"@type":"Organization","name":"Nikita Melkozerov","logo":{"@type":"ImageObject","url":"https://nikita.melkozerov.dev/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://nikita.melkozerov.dev/ accesskey=h title="Nikita Melkozerov (Alt + H)">Nikita Melkozerov</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu onscroll=menu_on_scroll()><li><a href=https://nikita.melkozerov.dev/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://nikita.melkozerov.dev/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://nikita.melkozerov.dev/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Image Classification from scratch without a PhD</h1><div class=post-description>Let's write a handwritten digit classifier from scratch, without prior ML experience</div><div class=post-meta>February 24, 2021&nbsp;·&nbsp;25 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><div class=details>Table of Contents</div></summary><div class=inner><ul><li><a href=#downloading-the-dataset aria-label="Downloading the dataset">Downloading the dataset</a></li><li><a href=#building-a-baseline-pixel-difference aria-label="Building a baseline: pixel difference">Building a baseline: pixel difference</a></li><li><a href=#machile-learning-time aria-label="Machile Learning time!">Machile Learning time!</a><ul><li><a href=#datasets aria-label=Datasets>Datasets</a></li><li><a href=#linear-model aria-label="Linear model">Linear model</a></li><li><a href=#everything-is-a-tensor aria-label="Everything is a Tensor">Everything is a Tensor</a></li><li><a href=#sgd aria-label=SGD>SGD</a></li><li><a href=#loss-function aria-label="Loss Function">Loss Function</a></li><li><a href=#training-loop aria-label="Training loop">Training loop</a></li></ul></li><li><a href=#recap aria-label=Recap>Recap</a></li><li><a href=#acknowledgements aria-label=Acknowledgements>Acknowledgements</a></li></ul></div></details></div><div class=post-content><p>Hello folks!</p><p>Recently I trained a handwritten digit classifier using the MNIST dataset from scratch, and it was an eye-opening experience for me. What looked like magic to me before now is just a few mathematical concepts applied together. I was very excited about how simple the whole process was, and I want to share it with everyone who still wonders what kind of magic is happening inside.</p><p>All you need to know is a bit of python and a few concepts from high-school math. No previous machine learning background is necessary.</p><p>This post is inspired by chapter 4 of the <a href=https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb>fast.ai book</a> (I highly recommend it if you&rsquo;re getting started with deep learning), where we tried to build a classifier that can recognize 3&rsquo;s and 7&rsquo;s from the MNIST dataset. This time, however, we will train a classifier to recognize all ten digits of the dataset. I will also try to avoid using &ldquo;magical&rdquo; high-level components as much as possible.</p><p>Let&rsquo;s go!</p><p>We will use PyTorch and fast.ai libraries for a few useful utilities.</p><p>First, we need to import a few functions from the fastai library:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> fastai.vision.all <span style=color:#f92672>import</span> <span style=color:#f92672>*</span>
<span style=color:#f92672>from</span> fastbook <span style=color:#f92672>import</span> <span style=color:#f92672>*</span>

matplotlib<span style=color:#f92672>.</span>rc(<span style=color:#e6db74>&#39;image&#39;</span>, cmap<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Greys&#39;</span>)
</code></pre></div><h2 id=downloading-the-dataset>Downloading the dataset<a hidden class=anchor aria-hidden=true href=#downloading-the-dataset>#</a></h2><p>Let&rsquo;s download the dataset first. Fast.ai has a convenient function to quickly obtain the dataset we&rsquo;re going to use for training:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>path <span style=color:#f92672>=</span> untar_data(URLs<span style=color:#f92672>.</span>MNIST)
path<span style=color:#f92672>.</span>ls()
</code></pre></div><pre><code>(#2) [Path('/home/nm/.fastai/data/mnist_png/training'),Path('/home/nm/.fastai/data/mnist_png/testing')]
</code></pre><p>The dataset has two folders inside it, <code>training</code> and <code>testing</code>. <code>training</code> should be used for the model training and validation, and <code>testing</code> used to compare the accuracy between different models.</p><blockquote><p>A small note: I will do a shortcut here and use the <code>testing</code> dataset to validate my model. That is something you shouldn&rsquo;t do in production when you want to evaluate the model&rsquo;s performance. Instead, you should split the training set into training/validation parts. For our toy problem, however, and for the sake of simplicity, I will use the testing set for validation.</p></blockquote><p>Let&rsquo;s peek inside the training folder:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>(path<span style=color:#f92672>/</span><span style=color:#e6db74>&#39;training&#39;</span>)<span style=color:#f92672>.</span>ls()
</code></pre></div><pre><code>(#10) [
    Path('/home/nm/.fastai/data/mnist_png/training/8'),
    Path('/home/nm/.fastai/data/mnist_png/training/3'),
    Path('/home/nm/.fastai/data/mnist_png/training/7'),
    Path('/home/nm/.fastai/data/mnist_png/training/5'),
    Path('/home/nm/.fastai/data/mnist_png/training/9'),
    Path('/home/nm/.fastai/data/mnist_png/training/4'),
    Path('/home/nm/.fastai/data/mnist_png/training/2'),
    Path('/home/nm/.fastai/data/mnist_png/training/6'),
    Path('/home/nm/.fastai/data/mnist_png/training/1'),
    Path('/home/nm/.fastai/data/mnist_png/training/0')
]
</code></pre><p>What we have here is ten folders, one for each digit. Every folder contains thousands of images of size 28x28 representing some digit.</p><p>You might say: &ldquo;Well, that&rsquo;s just a bunch of images compressed with a PNG algorithm. How can we do any math operations on them?&rdquo;. You&rsquo;re correct, plain image files are not very useful. To get started, we will first transform the images into a tensor.</p><p>&ldquo;Wait, what is a tensor?&rdquo; you ask, and I&rsquo;d say it a fancy name for a multi-dimensional array. The number of dimensions of this array is also called a <strong>rank of a tensor</strong>.</p><p>For example:</p><p>A zero-dimensional array (it is also called scalar) <code>1</code> is a tensor of rank 0.</p><p>1-d array aka vector <code>[1,2,3]</code> is a tensor of rank 1.</p><p>2-d arrray aka matrix</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>[[<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>], 
 [<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>6</span>],
 [<span style=color:#ae81ff>7</span>,<span style=color:#ae81ff>8</span>,<span style=color:#ae81ff>9</span>]]
</code></pre></div><p>is a tensor of rank 2, and so on.</p><p>We want to convert the images to tensors because then we can perform math operations on them.</p><p>Now we have an image, which is a 2-d array of pixels with pixel intensity values ranging from 0 to 255. Next, we will represent it as a tensor of a rank 2, and scale the pixel values between 0 and 1. Scaling the pixels between 0 and 1 will give us a convenient abstraction for some of the actions we will do next.</p><p>Once we created a tensor from one image, we will convert other images in our dataset to tensors as well. Then, we will stack the image tensors together into a single tensor of rank 3. I&rsquo;ll explain why we do this in a bit.</p><p>Let&rsquo;s write a function for this:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>image_path_to_tensor</span>(path):
    images_paths <span style=color:#f92672>=</span> path<span style=color:#f92672>.</span>ls()
    tensors <span style=color:#f92672>=</span> [tensor(Image<span style=color:#f92672>.</span>open(p)) <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> images_paths]
    <span style=color:#66d9ef>return</span> torch<span style=color:#f92672>.</span>stack(tensors)<span style=color:#f92672>.</span>float()<span style=color:#f92672>/</span><span style=color:#ae81ff>255</span>
</code></pre></div><p>Having defined the function, let&rsquo;s use it to transform all the images in 10 folders that we have:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>stacked_tensors <span style=color:#f92672>=</span> [image_path_to_tensor(image_folder) <span style=color:#66d9ef>for</span> image_folder <span style=color:#f92672>in</span> (path<span style=color:#f92672>/</span><span style=color:#e6db74>&#39;training&#39;</span>)<span style=color:#f92672>.</span>ls()<span style=color:#f92672>.</span>sorted()]
</code></pre></div><p>The result is ten tensors that represent the training set of each digit. Let&rsquo;s look at one digit and check the shape of the tensor:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>stacked_tensors[<span style=color:#ae81ff>4</span>]<span style=color:#f92672>.</span>shape
</code></pre></div><pre><code>torch.Size([5842, 28, 28])
</code></pre><p>The first dimension of the tensor corresponds to an image file, second and third dimensions are the height and the width of the image in pixels.</p><p>Because we had 5824 images for digit 4, and every image has the size of 28x28 pixels, therefore the resulting shape of the tensor is 5842 by 28 by 28. That&rsquo;s a tensor of rank 3.</p><h2 id=building-a-baseline-pixel-difference>Building a baseline: pixel difference<a hidden class=anchor aria-hidden=true href=#building-a-baseline-pixel-difference>#</a></h2><p>Before building any machine learning model it is generally a good idea to come up with a simple baseline first. One obvious choice would be a random baseline, where each digit is chosen at random, but let&rsquo;s try a little bit harder and build a baseline that relies on pixel similarity.</p><p>The idea is next: we will average the image tensors along the first axis, and it will be our &ldquo;pretty average digit&rdquo;. Then we will compare how similar each pixel of the image we try to classify to each pixel of every &ldquo;average&rdquo; digit, and our final guess will be the digit with the smallest error.</p><p>To start, let&rsquo;s compute our &ldquo;average&rdquo; digit images:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>image_means <span style=color:#f92672>=</span> [None] <span style=color:#f92672>*</span> <span style=color:#ae81ff>10</span>
<span style=color:#66d9ef>for</span> digit, imgs_tensor <span style=color:#f92672>in</span> enumerate(stacked_tensors):
    mean_tensor <span style=color:#f92672>=</span> imgs_tensor<span style=color:#f92672>.</span>mean(<span style=color:#ae81ff>0</span>)
    image_means[digit] <span style=color:#f92672>=</span> mean_tensor
</code></pre></div><p>For example, this is how an &ldquo;average&rdquo; 4 looks like:</p><p><img src=/images/image-classification-from-scratch/output_22_1.png#center alt=png></p><p>Now we need a function that can compute the error. One possible option is to subtract the pixels of two images, square them to make them between 0 and 1, and then take the mean of the result:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>calculate_error</span>(image, label):
    ideal_tensor <span style=color:#f92672>=</span> image_means[label]
    <span style=color:#66d9ef>return</span> ((image <span style=color:#f92672>-</span> ideal_tensor)<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>mean()
</code></pre></div><p>Let&rsquo;s try it out on a digit 3:</p><p><img src=/images/image-classification-from-scratch/output_25_1.png#center alt=png></p><p>The average digit of 3 looks like this:</p><p><img src=/images/image-classification-from-scratch/output_26_1.png#center alt=png></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>calculate_error(stacked_tensors[<span style=color:#ae81ff>3</span>][<span style=color:#ae81ff>0</span>], <span style=color:#ae81ff>3</span>)
</code></pre></div><pre><code>tensor(0.0632)
</code></pre><p>This function is also called mean squared error.</p><p>Now let&rsquo;s create a validation set. As I already mentioned above, using the testing set as a validation set is a shortcut for simplicity, and you should not do the same in production.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>valid_stacked <span style=color:#f92672>=</span> [image_path_to_tensor(image_folder) <span style=color:#66d9ef>for</span> image_folder <span style=color:#f92672>in</span> (path<span style=color:#f92672>/</span><span style=color:#e6db74>&#39;testing&#39;</span>)<span style=color:#f92672>.</span>ls()<span style=color:#f92672>.</span>sorted()]
</code></pre></div><p>We&rsquo;re able to compute an error for a single image. Our job now is to compute the error for all images in the validation set. Unfortunately, we can&rsquo;t simply write a loop and do this.</p><p><img src=/images/image-classification-from-scratch/not_simply.png#center alt="not simply meme"></p><p>The main issue is python loops are unlikely to be vectorized on the GPU. If we take a single image, put it on the GPU, calculate the error and then remove it from the GPU, it will take us quite a while to calculate the error for the whole validation set.</p><p>Instead of using a loop, we will grab the tensor with images, put it on the GPU, and then calculate the error for all images in parallel. GPUs are super fast number-crunching machines, and it will take no time for our GPU to do that.</p><p>But how can we do this? Welcome to broadcasting. Broadcasting is a technique that allows PyTorch to perform operations on tensors with different shapes in a way as they were same-shape tensors. Let me explain it with a simple example.</p><p>If we have two tensors:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>a <span style=color:#f92672>=</span> [[<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>],
     [<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>],
     [<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>6</span>]]
</code></pre></div><p>and</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>b <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>1</span>]
</code></pre></div><p>then if we write something like <code>c = a + b</code>, we will get the next tensor as a result:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>c <span style=color:#f92672>=</span> [[<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>],
     [<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>],
     [<span style=color:#ae81ff>6</span>,<span style=color:#ae81ff>7</span>]]
</code></pre></div><p>Tensor <code>b</code> was added to each element of the tensor <code>a</code> even though they have different ranks. Simply speaking, instead of looping through all elements of the vector <code>a</code>, we told PyTorch to do this in a declarative fashion.</p><p>With broadcasting in mind, let&rsquo;s define a few functions to measure the distances between our images.</p><p>Naïve approach with subtracting one image pixes from another and taking the mean will not work very well.
It is possible to have drastically different images where dark and bright pixels will compensate each other, and the mean will be close to zero.
To solve this problem, we should make the difference positive and then take the mean.</p><p>For example, we can calculate a mean absolute error (MAE) and a mean squared error (MSE). The difference between them is that MSE would result in a higher error when images are very different.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>mnist_distance_mae</span>(a,b): <span style=color:#66d9ef>return</span> (a<span style=color:#f92672>-</span>b)<span style=color:#f92672>.</span>abs()<span style=color:#f92672>.</span>mean((<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>,<span style=color:#f92672>-</span><span style=color:#ae81ff>2</span>))
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>mnist_distance_mse</span>(a,b): <span style=color:#66d9ef>return</span> torch<span style=color:#f92672>.</span>square((a<span style=color:#f92672>-</span>b))<span style=color:#f92672>.</span>mean((<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>,<span style=color:#f92672>-</span><span style=color:#ae81ff>2</span>))
</code></pre></div><p>The functions above subtract pixels of two images, then take the absolute/squared value and computing the mean. <code>(-1,-2)</code> means that we want to compute the mean along the last and last-1 axis of our tensor, which corresponds to 28x28 image.</p><blockquote><p>When I was implementing this code, broadcasting led to some very nasty bugs. For example, I had a situation where my model was not able to make any progress in training. Another time the training was much slower than I expected, and I did not know why. As it turned out, there was a bug in the loss function because of implicit broadcasting, and it took me a lot of time to localize the problem.
It is very important to check that the shape of the returned tensor on every stage corresponds to what we expect.</p></blockquote><p>Let&rsquo;s see what is the shape of the result of our distance function:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>mnist_distance_mae(valid_stacked[<span style=color:#ae81ff>3</span>], image_means[<span style=color:#ae81ff>3</span>])<span style=color:#f92672>.</span>shape
</code></pre></div><pre><code>torch.Size([1010])
</code></pre><p>It is a tensor of size 1010, with one distance per one validation image. That seems to be correct, and we can continue.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>mnist_distance_mse(valid_stacked[<span style=color:#ae81ff>3</span>], image_means[<span style=color:#ae81ff>3</span>])
</code></pre></div><pre><code>tensor([0.0575, 0.0516, 0.0542,  ..., 0.0480, 0.0642, 0.0429])
</code></pre><p>Given the distance functions, now we can write a function which will tell us if our digit prediction is correct:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>analyze_digit</span>(digit: str, candidate, distance_function):
    distances <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>stack([distance_function(candidate, mean_tensor) <span style=color:#66d9ef>for</span> mean_tensor <span style=color:#f92672>in</span> image_means])
    lowest_distance <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>argmin(distances, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
    <span style=color:#66d9ef>return</span> lowest_distance <span style=color:#f92672>==</span> digit
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>analyze_digit(<span style=color:#ae81ff>4</span>, stacked_tensors[<span style=color:#ae81ff>3</span>][<span style=color:#ae81ff>113</span>], mnist_distance_mae)
</code></pre></div><pre><code>tensor(False)
</code></pre><p>Let&rsquo;s put broadcasting into work and do analyze all 3s at once:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>analyze_digit(<span style=color:#ae81ff>3</span>, valid_stacked[<span style=color:#ae81ff>3</span>], mnist_distance_mae)<span style=color:#f92672>.</span>float()<span style=color:#f92672>.</span>mean()
</code></pre></div><pre><code>tensor(0.6089)
</code></pre><p>Our accuracy is about 60%. It is better than random (it would be around 10), but we can do better. What is the mean accuracy for all digits that we have? We can use both of our distance functions and compare the results:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>accuracy_mae <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>stack([analyze_digit(digit, valid_stacked[digit], mnist_distance_mae)<span style=color:#f92672>.</span>float()<span style=color:#f92672>.</span>mean() <span style=color:#66d9ef>for</span> digit <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span>)])
accuracy_mae
</code></pre></div><pre><code>tensor([0.8153, 0.9982, 0.4234, 0.6089, 0.6680, 0.3262, 0.7871, 0.7646, 0.4425, 0.7760])
</code></pre><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>accuracy_mse <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>stack([analyze_digit(digit, valid_stacked[digit], mnist_distance_mse)<span style=color:#f92672>.</span>float()<span style=color:#f92672>.</span>mean() <span style=color:#66d9ef>for</span> digit <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span>)])
accuracy_mse
</code></pre></div><pre><code>tensor([0.8959, 0.9621, 0.7568, 0.8059, 0.8259, 0.6861, 0.8633, 0.8327, 0.7372, 0.8067])
</code></pre><p>As we see, MSE gives us better results since it penalizes bigger differences more compared to smaller ones. Let&rsquo;s look at the mean accuracies:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>accuracy_mae<span style=color:#f92672>.</span>mean()
</code></pre></div><pre><code>tensor(0.6610)
</code></pre><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>accuracy_mse<span style=color:#f92672>.</span>mean()
</code></pre></div><pre><code>tensor(0.8173)
</code></pre><p>One observation is that MSE gives us better accuracy compared to MAE.</p><p>We got 82% accuracy without any learning at all! We can take it as our baseline, and our goal would be to train a machine learning model which can do this better. It is always a good idea to start with a simple solution and then move to a more sophisticated one if the observed performance is below expectations.</p><h2 id=machile-learning-time>Machile Learning time!<a hidden class=anchor aria-hidden=true href=#machile-learning-time>#</a></h2><p>Now, we&rsquo;re finally going to do some machine learning.</p><h3 id=datasets>Datasets<a hidden class=anchor aria-hidden=true href=#datasets>#</a></h3><p>First, we will concatenate our images for different digits into a single tensor, and then we will represent every 28x28 image as 1x784 vector. I&rsquo;ll explain why we do it in a bit. This tensor is going to be the &ldquo;input&rdquo; for our model.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>train_x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat([stacked_tensors[i] <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span>)])<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>28</span><span style=color:#f92672>*</span><span style=color:#ae81ff>28</span>)
</code></pre></div><p>Next, let&rsquo;s create another tensor containing the correct labels for every image in our previous tensor.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>train_y <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat([torch<span style=color:#f92672>.</span>stack([tensor(i)]<span style=color:#f92672>*</span>len(stacked_tensors[i])) <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span>)])
train_x<span style=color:#f92672>.</span>shape,train_y<span style=color:#f92672>.</span>shape
</code></pre></div><pre><code>(torch.Size([60000, 784]), torch.Size([60000]))
</code></pre><p>Finally, we will combine them in a dataset. A dataset is just a simple pair of inputs and outputs to the model, in our case, it is the images (train_x) and the labels (train_y).</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>dataset <span style=color:#f92672>=</span> list(zip(train_x,train_y))
</code></pre></div><p>We can also look at what is inside. To do this, we want to convert 1x784 tensor into a 28x28 one. Method <code>view</code> of a tensor can help us with it:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>show_image(dataset[<span style=color:#ae81ff>32000</span>][<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>28</span>,<span style=color:#ae81ff>28</span>))
</code></pre></div><p><img src=/images/image-classification-from-scratch/output_65_1.png#center alt=png></p><p>And the label is also 5:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>dataset[<span style=color:#ae81ff>32000</span>][<span style=color:#ae81ff>1</span>]
</code></pre></div><pre><code>tensor(5)
</code></pre><p>Now we perform the same operations for the validation set:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>valid_x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat([valid_stacked[i] <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span>)])<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>28</span><span style=color:#f92672>*</span><span style=color:#ae81ff>28</span>)
valid_y <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat([torch<span style=color:#f92672>.</span>stack([tensor(i)]<span style=color:#f92672>*</span>len(valid_stacked[i])) <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span>)])
valid_x<span style=color:#f92672>.</span>shape,valid_y<span style=color:#f92672>.</span>shape
</code></pre></div><pre><code>(torch.Size([10000, 784]), torch.Size([10000]))
</code></pre><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>valid_dset <span style=color:#f92672>=</span> list(zip(valid_x,valid_y))
</code></pre></div><h3 id=linear-model>Linear model<a hidden class=anchor aria-hidden=true href=#linear-model>#</a></h3><p>Before we continue, let&rsquo;s quickly talk about what we have done and what we&rsquo;re going to do next.</p><p>We have already collected the training and the validation datasets, and the dataset is just a collection of images and the corresponding labels. We also transformed every image into a vector.</p><p>What we need now is to define a function, which will take an image as an input and will produce the prediction as an output.</p><p>To be more specific, our function will accept a vector of size 784 as an input, where each element of this vector will correspond to a pixel of an image. Since our function does not care about the arrangement of input values, we reshaped our tensor earlier to make it more convenient for us to work with it. The produced result will be a vector of size 10, where each number will represent the probability of each digit.</p><p>In the code, it would be something like this:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict</span>(pixels: List[float]): List[float]
</code></pre></div><p>But what kind of function is capable of doing such a transformation? To our luck, such functions exist, and they are called neural networks!</p><p>It is proven that given enough parameters, a neural network with only one hidden layer is capable of approximating any function with an arbitrary level of precision. How cool is that! For more information, look for the universal approximation theorem.</p><p>We, however, will start with a degenerate case of a neural network called a linear function.</p><p>In pseudocode it looks like this:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>linear</span>(x: float, weight: float, bias: float) <span style=color:#f92672>-&gt;</span> float:
    <span style=color:#66d9ef>return</span> x <span style=color:#f92672>*</span> weight <span style=color:#f92672>+</span> bias
</code></pre></div><p>This function accepts an input, number <code>x</code>, a parameter called <code>weight</code>, and a parameter called <code>bias</code>. Then the input is multiplied by the weight and the bias added in the end. Sounds simple, right?</p><p>However, in our case, we have not a single but 784 input parameters. Let&rsquo;s for now pretend that we&rsquo;re only interested in predicting a single digit:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> typing <span style=color:#f92672>import</span> List
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>linear</span>(x: List[float], weights: List[float], bias: float) <span style=color:#f92672>-&gt;</span> float:
    <span style=color:#66d9ef>return</span> (x <span style=color:#f92672>*</span> weights)<span style=color:#f92672>.</span>sum() <span style=color:#f92672>+</span> bias
</code></pre></div><p>Here <code>*</code> stand for element-wise vector multiplication, meaning that if we have two vectors,</p><pre><code>a = [1, 2, 3]
b = [1, 2, 3]
</code></pre><p>then <code>a * b</code> would be the vector <code>[1*1, 2*2, 3*3]</code></p><p>The result of this function will be a likeliness that our input x corresponds to a label. But what if we want to predict more than one label?</p><p>That&rsquo;s simple as well. First, we define a few sets of weights and biases, one per class we want to predict. Then we call the function several times to make a prediction for each class. The number returned by the functions represents neural net confidence in the predicted digit. Then we interpret the result with the highest number as the predicted class. Let&rsquo;s say we only want to predict three digits, then what we want to do is next:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>score_0 <span style=color:#f92672>=</span> linear(image, weights_0, bias_0)
score_1 <span style=color:#f92672>=</span> linear(image, weights_1, bias_1)
score_2 <span style=color:#f92672>=</span> linear(image, weights_2, bias_2)
</code></pre></div><p>Finally, we check which resulted in the highest score, and this is going to be the predicted class.</p><p>Let&rsquo;s wrap it in a function:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict</span>(image: List[float], weights: List[List[float]], biases: List[float]) <span style=color:#f92672>-&gt;</span> List[float]:
    results <span style=color:#f92672>=</span> []
    <span style=color:#66d9ef>for</span> digit <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>3</span>):
        likeliness <span style=color:#f92672>=</span> linear(image, weights[digit], biases[digit])
        results<span style=color:#f92672>.</span>append(likeliness)
    <span style=color:#66d9ef>return</span> results
</code></pre></div><p>That&rsquo;s it! Our simple linear model will be able to predict an image from pixels.</p><p>However, we have two problems:</p><ol><li>This function is going to be extremely slow since GPUs do not like loops, and the loop we defined earlier will be executed using plain python runtime.</li><li>We don&rsquo;t know where to get these magical weights and biases!</li></ol><p>To deal with the first problem, we will utilize PyTorch tensors, which will give us free GPU parallelization.</p><p>As for the second problem, we need to learn about <strong>Stochastic Gradient Descent</strong> (SGD).</p><h3 id=everything-is-a-tensor>Everything is a Tensor<a hidden class=anchor aria-hidden=true href=#everything-is-a-tensor>#</a></h3><p>Since we want to use GPU for computation, we need to represent our weights and biases as tensors. Let&rsquo;s write a helper function for this.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>init_params</span>(size): <span style=color:#66d9ef>return</span> torch<span style=color:#f92672>.</span>randn(size)<span style=color:#f92672>.</span>requires_grad_()
</code></pre></div><p>Method <code>.requires_grad_()</code> tells PyTorch to track the operations done with the tensor so that later we can ask PyTorch to compute gradients for us. More on that in a bit.</p><p>Remember the weights in the <code>predict</code> function? We wanted to have something like this <code>List[List[float; 784]; 10]</code> (The number next to the type is the size of the list). That is nothing else than a matrix of size 784 by 10.</p><p>If you&rsquo;re wondering where we got these magic numbers, it comes from the following. The number 784 used because our input image has a size of 28x28, and later we unroll it into a 1x784 vector. Number 10 comes from the ten classes of digits we&rsquo;re going to recognize.</p><p>Let&rsquo;s initialize the weights randomly:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>weights <span style=color:#f92672>=</span> init_params((<span style=color:#ae81ff>28</span><span style=color:#f92672>*</span><span style=color:#ae81ff>28</span>,<span style=color:#ae81ff>10</span>))
</code></pre></div><p>The same goes for biases:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>biases <span style=color:#f92672>=</span> init_params(<span style=color:#ae81ff>10</span>)
</code></pre></div><p>Now we can try to calculate the predictions for a single image. Instead of looping for every digit and multiplying vectors, we can do everything in one single step using matrix-vector multiplication. And PyTorch will use the power of the GPU to do this operation as quickly as possible!</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>train_x[<span style=color:#ae81ff>34000</span>] <span style=color:#960050;background-color:#1e0010>@</span> weights <span style=color:#f92672>+</span> biases
</code></pre></div><pre><code>tensor([  5.4221,  -1.5908,  17.0564,  -4.3119, -15.0097,  -8.3001,   0.5106,   2.3884,  -6.6653,  13.2021], grad_fn=&lt;AddBackward0&gt;)
</code></pre><p>The new operator <code>@</code> above is a PyTorch operator for matrix multiplication. I will not get into details about what matrix multiplication is, but a nice and intuitive explanation of it can be found on <a href=http://matrixmultiplication.xyz/>http://matrixmultiplication.xyz/</a>.</p><p>Let&rsquo;s write a function that will represent our linear model</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>linear_model</span>(x_batch): 
    <span style=color:#66d9ef>return</span> x_batch <span style=color:#960050;background-color:#1e0010>@</span> weights <span style=color:#f92672>+</span> biases
predictions <span style=color:#f92672>=</span> linear_model(train_x[<span style=color:#ae81ff>34000</span>])
predictions
</code></pre></div><pre><code>tensor([  5.4221,  -1.5908,  17.0564,  -4.3119, -15.0097,  -8.3001,   0.5106,   2.3884,  -6.6653,  13.2021], grad_fn=&lt;AddBackward0&gt;)
</code></pre><p>One interesting property of our matrix-powered function is that this function can process a batch of several images at once.
Instead of providing a single vector as an input, we can provide a matrix containing a few images, then do the matrix multiplication, and then we will get the matrix with the predictions back.</p><p>With this in mind, we can calculate predictions for the whole dataset without writing a single loop:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>predictions <span style=color:#f92672>=</span> linear_model(train_x)
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>predictions<span style=color:#f92672>.</span>shape
</code></pre></div><pre><code>torch.Size([60000, 10])
</code></pre><p><code>argmax</code> will return the index of the tensor with the biggest value. Since we expect the model to return bigger numbers for the predictions it is more confident with, the number returned will be the predicted class. Let&rsquo;s calculate the accuracy:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>(predictions<span style=color:#f92672>.</span>argmax(dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>) <span style=color:#f92672>==</span> train_y)<span style=color:#f92672>.</span>float()<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>item()

</code></pre></div><pre><code>0.12043333053588867
</code></pre><p>The model initialized using random parameters made around 10% of predictions correct. That&rsquo;s pretty much what we would expect for a random model since the probability of being correct with a random guess is 0.1.</p><h3 id=sgd>SGD<a hidden class=anchor aria-hidden=true href=#sgd>#</a></h3><p>So we solved the first problem, the prediction function is fast and can be run on a GPU. But we still have no idea where to get the proper (non-random) parameters.</p><p>Now, let&rsquo;s talk about gradient descent (the &ldquo;stochastic&rdquo; part will be explained later).</p><p>As you might remember from a calculus course, a derivative of a function is another function that represents a rate of change of a function output with respect to its input. If we evaluate a derivative at some point where our function is defined, we can think of a result as a direction in which the function evaluated at that point will grow the fastest. If we know this, we can easily figure out in which direction we need to go in order to reach the minimum of the function.</p><p>In case our function operates on several input variables, instead of the derivative we will calculate a gradient, which can be imagined as an &ldquo;advanced&rdquo; version of the derivative capable of working with multi-variable functions. The gradient is similar to the derivative since it will give us the direction in which the function is going to grow the fastest.</p><p>Once we have the gradient, we can multiply it by -1 to get the direction in which the function will decline the fastest, then multiply the gradient by a small value and update the parameters. After we do this, the function we&rsquo;re interested in will produce a smaller value given the updated parameters.</p><p>Using this information, we can describe the gradient descent process as following:</p><pre><code>initialize function parameter randomly
while (function parameters are not good enough):
    calculate gradient
    parameters = parameters - gradient * step
</code></pre><p>The gradient descent is capable of optimizing any differentiable function, and we will utilize this for our training process.</p><p>Next, we need to define a so-called loss function, which will tell us how good or bad the performance of our linear model is when we change the parameters.</p><p>Once we define it, we will run the gradient descent method with the loss function and find the optimal parameters.</p><h3 id=loss-function>Loss Function<a hidden class=anchor aria-hidden=true href=#loss-function>#</a></h3><p>Let&rsquo;s think about what we want from our loss function. We want it to take images, parameters, and labels as an input and produce a number that shows how &ldquo;bad&rdquo; our linear model is.</p><p>Taking this into account, our first implementation of the loss function will look like this:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>mnist_loss</span>(predictions, targets, nr_classes<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>):
    predictions <span style=color:#f92672>=</span> predictions<span style=color:#f92672>.</span>sigmoid()
    mask <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>one_hot(targets, nr_classes)
    errors <span style=color:#f92672>=</span> ((mask<span style=color:#f92672>-</span>predictions) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)
    <span style=color:#66d9ef>return</span> errors<span style=color:#f92672>.</span>mean()
</code></pre></div><p>Let&rsquo;s analyze it line by line:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>predictions <span style=color:#f92672>=</span> predictions<span style=color:#f92672>.</span>sigmoid()
</code></pre></div><p>We apply a sigmoid function to each prediction. It looks like this:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>sigmoid</span>(x): <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>1</span><span style=color:#f92672>/</span>(<span style=color:#ae81ff>1</span><span style=color:#f92672>+</span>torch<span style=color:#f92672>.</span>exp(<span style=color:#f92672>-</span>x))
</code></pre></div><p>The sigmoid function maps its input to a range between 0 and 1, and it looks like this:</p><p><img src=/images/image-classification-from-scratch/output_111_1.png#center alt=png></p><p>Our idea is to subtract correct predictions from 1-s, so that when our model is correct and confident in the prediction, the error will be close to zero. With other predictions, we will not do anything, which will result in a low error of the model does not give us high confidence for incorrect labels.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>mask <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>one_hot(targets, nr_classes)
</code></pre></div><p>Here we generate a mask, which will represent the correct labels. For example <code>F.one_hot(tensor([0,2,1]), 3)</code> will give us</p><pre><code>[[1,0,0],
 [0,0,1],
 [0,1,0]]
</code></pre><p>Next, we will subtract our prediction from the mask. To deal with negative numbers we will take a squared value of the error, which will keep the function differentiable and additionally will have a nice bonus of punishing big errors more than small ones.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>errors <span style=color:#f92672>=</span> ((mask<span style=color:#f92672>-</span>predictions) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)
</code></pre></div><p>In the end, we will take a mean to reduce the error tensor to a single number:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>errors<span style=color:#f92672>.</span>mean()
</code></pre></div><p>Having defined a lost function, we want to calculate its gradient. However, we run into a problem: our loss function operates on predictions and labels and not on the model parameters.</p><p>To include the weights and biases into the gradient calculating, we will take a derivative of a new function which is a composition of the model and the loss function. The new function will look like this:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>composed</span>(x_batch, y_batch):
    preds <span style=color:#f92672>=</span> linear_model(x_batch)
    loss <span style=color:#f92672>=</span> mnist_loss(preds, y_batch)
</code></pre></div><p>Now we need to calculate a gradient. If we do that by hand via calculating partial derivatives using a chain rule, the process would be a bit tedious because this function operates on thousands of parameters.</p><p>Fortunately for us, PyTorch can solve this quickly. What we need to do is to call <code>.requires_grad()</code> on a tensor in which gradients we&rsquo;re interested before computing the function, and then call <code>.backward()</code> to tell PyTorch that we want the gradients.</p><p>Here is how we do it:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>calc_grad</span>(x_batch, y_batch, model, loss_fn):
    preds <span style=color:#f92672>=</span> model(x_batch)
    loss <span style=color:#f92672>=</span> loss_fn(preds, y_batch)
    loss<span style=color:#f92672>.</span>backward()
</code></pre></div><p>Let&rsquo;s talk about how we will feed the data to our gradient descent process. One option is to feed it one image at a time, but this will take an unreasonable amount of time to complete due to data moving overhead. Training on the whole dataset at once is not desirable either, since the dataset might be simply too big to fit on the GPU. What is commonly done is the following: the dataset is divided in batches, and then we calculate gradients using the whole batch. That&rsquo;s where the word stochastic comes from, mainly because batched contain shuffled items, and the gradient descent is run on data randomly sampled from the dataset.</p><p>When we iterate through the dataset, we generally prefer to have diverse examples in our batches because this leads to better generalization. An easy way to achieve this is to shuffle the dataset. Fortunately for us, PyTorch provides a class called DataLoader, which does the shuffling and batch separation for us:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>dl <span style=color:#f92672>=</span> DataLoader(dataset, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>256</span>,shuffle<span style=color:#f92672>=</span>True)
valid_dl <span style=color:#f92672>=</span> DataLoader(valid_dset, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>256</span>, shuffle<span style=color:#f92672>=</span>False)
</code></pre></div><h3 id=training-loop>Training loop<a hidden class=anchor aria-hidden=true href=#training-loop>#</a></h3><p>We can also write a function that runs SGD using the whole dataset (an iteration through all images called an epoch):</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_epoch</span>(model, loss_fn, lr, params):
    <span style=color:#66d9ef>for</span> xb,yb <span style=color:#f92672>in</span> dl:
        calc_grad(xb, yb, model, loss_fn)
        <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> params:
            p<span style=color:#f92672>.</span>data <span style=color:#f92672>-=</span> p<span style=color:#f92672>.</span>grad <span style=color:#f92672>*</span> lr
            p<span style=color:#f92672>.</span>grad<span style=color:#f92672>.</span>zero_()
</code></pre></div><p>For every batch, we compute the gradients, multiply them by a small number called the learning rate, and subtract the result from our initial parameters. The learning rate should be low enough to keep the process stable and at the same time large enough so that our training does not last forever.</p><p>It is important to know how accurate our model is. We will start with the accuracy of a single batch:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>batch_accuracy</span>(xb, yb):
    correct <span style=color:#f92672>=</span> (xb<span style=color:#f92672>.</span>argmax(axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>) <span style=color:#f92672>==</span> yb)<span style=color:#f92672>.</span>float()<span style=color:#f92672>.</span>mean()
    <span style=color:#66d9ef>return</span> correct
</code></pre></div><p>Once we know how to measure the accuracy of the batch, we can measure the accuracy of the model in a single epoch:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>validate_epoch</span>(model):
    accs <span style=color:#f92672>=</span> [batch_accuracy(model(xb), yb) <span style=color:#66d9ef>for</span> xb,yb <span style=color:#f92672>in</span> valid_dl]
    <span style=color:#66d9ef>return</span> round(torch<span style=color:#f92672>.</span>stack(accs)<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>item(), <span style=color:#ae81ff>4</span>)
</code></pre></div><p>Let&rsquo;s see what the accuracy of our randomly-initialized model is:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>validate_epoch(linear_model)
</code></pre></div><pre><code>0.1272
</code></pre><p>Something around 10% is what we expect.</p><p>Finally, we can train a single epoch:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>accuracy <span style=color:#f92672>=</span> []
lr <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.</span>
<span style=color:#75715e># initializing random weights</span>
weights <span style=color:#f92672>=</span> init_params((<span style=color:#ae81ff>28</span><span style=color:#f92672>*</span><span style=color:#ae81ff>28</span>,<span style=color:#ae81ff>10</span>))
<span style=color:#75715e># initializing random biases</span>
biases <span style=color:#f92672>=</span> init_params(<span style=color:#ae81ff>10</span>)

params <span style=color:#f92672>=</span> weights,biases
<span style=color:#75715e># training an epoch</span>
train_epoch(linear_model, mnist_loss, lr, params)
<span style=color:#75715e># validating results</span>
accuracy<span style=color:#f92672>.</span>append(validate_epoch(linear_model))
accuracy[<span style=color:#ae81ff>0</span>]
</code></pre></div><pre><code>0.1405
</code></pre><p>Let&rsquo;s train for 40 epochs:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>40</span>):
    train_epoch(linear_model, mnist_loss, lr, params)
    accuracy<span style=color:#f92672>.</span>append(validate_epoch(linear_model))

plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>6</span>))
plt<span style=color:#f92672>.</span>plot(accuracy)
plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;accuracy&#39;</span>)
plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;epoch&#39;</span>)
plt<span style=color:#f92672>.</span>show()
</code></pre></div><p><img src=/images/image-classification-from-scratch/output_132_0.png#center alt=png></p><p>The training starts fast, but it slows down as model accuracy increases. The problem is our model can be very certain about several different classes for the same image. However, this is not the task we&rsquo;re trying to solve.
The reason for this behavior lies in our loss function. Remember, the loss function first applies sigmoid to keep the function output between 0 and 1. This only scales the output, but what we want is that the model will select one label at the end.</p><p>Let&rsquo;s replace the sigmoid function with a so-called softmax function. It is similar to sigmoid when we&rsquo;re predicting a single class, but it will make the output probabilities sum to 1 if we have more than one class. Thus, only the relative difference of the predictions will be important, and a high degree of confidence in one class will automatically decrease confidence in other classes.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>mnist_loss_softmax</span>(predictions, targets, nr_classes<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>):
    predictions <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>softmax(predictions, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
    mask <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>one_hot(targets, nr_classes)
    errors <span style=color:#f92672>=</span> ((mask<span style=color:#f92672>-</span>predictions) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)
    <span style=color:#66d9ef>return</span> errors<span style=color:#f92672>.</span>mean()
</code></pre></div><p>Let&rsquo;s train the model again:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>accuracy <span style=color:#f92672>=</span> []

lr <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.</span>
<span style=color:#75715e># initializing random weights</span>
weights <span style=color:#f92672>=</span> init_params((<span style=color:#ae81ff>28</span><span style=color:#f92672>*</span><span style=color:#ae81ff>28</span>,<span style=color:#ae81ff>10</span>))
<span style=color:#75715e># initializing random biases</span>
biases <span style=color:#f92672>=</span> init_params(<span style=color:#ae81ff>10</span>)

params <span style=color:#f92672>=</span> weights,biases
<span style=color:#75715e># training an epoch</span>
train_epoch(linear_model, mnist_loss_softmax, lr, params)
<span style=color:#75715e># validating results</span>
accuracy<span style=color:#f92672>.</span>append(validate_epoch(linear_model))
accuracy[<span style=color:#ae81ff>0</span>]
</code></pre></div><pre><code>0.1619
</code></pre><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>40</span>):
    train_epoch(linear_model, mnist_loss_softmax, lr, params)
</code></pre></div><p><img src=/images/image-classification-from-scratch/output_137_0.png#center alt=png></p><p>That helped, but our learning is still getting slower as the accuracy is getting higher, and we have not yet surpassed our baseline.</p><p>Let&rsquo;s blame the loss function again! Now we can think of a problem we face as following: for our loss function the absolute difference between parameters that give 0.9 and 0.99 accuracies is small, about 0.1. But if we think about it, the second set would give us 10x more accurate results! To address this problem, we&rsquo;re going to apply a negative logarithm fucntion to the results. It will rescale our outputs in a way so the SGD will find the right direction easier.</p><p>The negative logarithm function looks like this:</p><p><img src=/images/image-classification-from-scratch/output_139_0.png#center alt=png></p><p>Such a loss function is then called a cross-entropy loss:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>cross_entropy_loss</span>(preds, y):
    <span style=color:#75715e># apply softmax</span>
    preds <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>softmax(preds, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)

    <span style=color:#75715e># get confidences for the correct class</span>
    idx <span style=color:#f92672>=</span> len(preds)
    confidences <span style=color:#f92672>=</span> preds[range(idx), y]

    <span style=color:#75715e># calculate negative log likelihood and return its mean</span>
    log_ll <span style=color:#f92672>=</span>  <span style=color:#f92672>-</span>torch<span style=color:#f92672>.</span>log(confidences)
    <span style=color:#66d9ef>return</span> log_ll<span style=color:#f92672>.</span>mean()
</code></pre></div><p>Let&rsquo;s train the model again using the new shiny loss function:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>accuracy <span style=color:#f92672>=</span> []
lr <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.</span>
<span style=color:#75715e># initializing random weights</span>
weights <span style=color:#f92672>=</span> init_params((<span style=color:#ae81ff>28</span><span style=color:#f92672>*</span><span style=color:#ae81ff>28</span>,<span style=color:#ae81ff>10</span>))
<span style=color:#75715e># initializing random biases</span>
biases <span style=color:#f92672>=</span> init_params(<span style=color:#ae81ff>10</span>)

params <span style=color:#f92672>=</span> weights,biases
<span style=color:#75715e># training an epoch</span>
train_epoch(linear_model, cross_entropy_loss, lr, params)
<span style=color:#75715e># validating results</span>
accuracy<span style=color:#f92672>.</span>append(validate_epoch(linear_model))
accuracy[<span style=color:#ae81ff>0</span>]
</code></pre></div><pre><code>0.8431
</code></pre><p>Wow, already after training for a single epoch we got 85% accuracy, outperforming our baseline. That&rsquo;s a huge improvement! Let&rsquo;s train for a few more epochs:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>40</span>):
    train_epoch(linear_model, cross_entropy_loss, lr, params)
    accuracy<span style=color:#f92672>.</span>append(validate_epoch(linear_model))

plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>6</span>))
plt<span style=color:#f92672>.</span>plot(accuracy)
plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;accuracy&#39;</span>)
plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;epoch&#39;</span>)
plt<span style=color:#f92672>.</span>show()
</code></pre></div><p><img src=/images/image-classification-from-scratch/output_145_0.png#center alt=png></p><p>91.5% accuracy with a simple linear model! Isn&rsquo;t it impressive?</p><p>But so far out model can hardly be called a neural network since it consists of a single linear layer. To make it a &ldquo;real&rdquo; neural net, we should add some-non-linearity. For example, we can add a sigmoid layer in-between two linear layers and then use function composition to apply them sequentially:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>w1 <span style=color:#f92672>=</span> init_params((<span style=color:#ae81ff>28</span><span style=color:#f92672>*</span><span style=color:#ae81ff>28</span>,<span style=color:#ae81ff>64</span>))
b1 <span style=color:#f92672>=</span> init_params(<span style=color:#ae81ff>64</span>)
w2 <span style=color:#f92672>=</span> init_params((<span style=color:#ae81ff>64</span>,<span style=color:#ae81ff>10</span>))
b2 <span style=color:#f92672>=</span> init_params(<span style=color:#ae81ff>10</span>)

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>simple_neural_net</span>(xb): 
    res <span style=color:#f92672>=</span> xb<span style=color:#a6e22e>@w1</span> <span style=color:#f92672>+</span> b1 <span style=color:#75715e># 1st linear layer</span>
    res <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>sigmoid(res) <span style=color:#75715e># 2nd non-linear layer</span>
    res <span style=color:#f92672>=</span> res<span style=color:#a6e22e>@w2</span> <span style=color:#f92672>+</span> b2 <span style=color:#75715e># 3rd linear layer</span>
    <span style=color:#66d9ef>return</span> res

params <span style=color:#f92672>=</span> [w1,b1,w2,b2]
accuracy <span style=color:#f92672>=</span> []
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>lr <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.</span>

<span style=color:#75715e># training an epoch</span>
train_epoch(simple_neural_net, cross_entropy_loss, lr, params)
<span style=color:#75715e># validating results</span>
validate_epoch(simple_neural_net)
</code></pre></div><pre><code>0.8372
</code></pre><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>40</span>):
    train_epoch(simple_neural_net, cross_entropy_loss, lr, params)
</code></pre></div><p><img src=/images/image-classification-from-scratch/output_149_0.png#center alt=png></p><p>Around 95% accuracy! That&rsquo;s definitely better compared to the plain linear model.</p><p>As you can see, our new non-linear model is a drop-in replacement for the old one (as long as it has the same number of inputs and outputs), and the rest of the process stays exactly the same.</p><p>Finally, as a fun example let&rsquo;s see how this task can be done using a neural network designed for image recognition:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>dls <span style=color:#f92672>=</span> ImageDataLoaders<span style=color:#f92672>.</span>from_folder(path, train<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;training&#34;</span>, valid<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;testing&#34;</span>)
learn <span style=color:#f92672>=</span> cnn_learner(dls, resnet18, pretrained<span style=color:#f92672>=</span>False,
                    loss_func<span style=color:#f92672>=</span>F<span style=color:#f92672>.</span>cross_entropy, metrics<span style=color:#f92672>=</span>accuracy)
learn<span style=color:#f92672>.</span>fit_one_cycle(<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>0.1</span>)
</code></pre></div><table><thead><tr><th>epoch</th><th>train_loss</th><th>valid_loss</th><th>accuracy</th><th>time</th></tr></thead><tbody><tr><td>0</td><td>1.519870</td><td>1.372195</td><td>0.914800</td><td>00:33</td></tr><tr><td>1</td><td>0.698865</td><td>53.716496</td><td>0.968600</td><td>00:32</td></tr><tr><td>2</td><td>0.167927</td><td>0.049071</td><td>0.988100</td><td>00:33</td></tr><tr><td>3</td><td>0.031542</td><td>0.020860</td><td>0.994100</td><td>00:34</td></tr></tbody></table><p>Wow, 99.4% only after four epochs! Those modern neural networks are powerful.</p><p>It is worth mentioning that SGD is not ideal. For example, there are no guarantees that it actually finds good parameters <em>in a finite number of steps</em>, let alone optimal ones.</p><p>Also, a neural network is a non-convex function, so there may be many local minimums where the SGD can get stuck.
This is why simpler methods are often better suited to problems that can be solved with the indicated simpler methods.</p><p>Finally, ML researchers have made significant progress in the area of SGD, and neural networks and modern SGD + regularization have achieved state-of-the-art results for many problems.</p><h2 id=recap>Recap<a hidden class=anchor aria-hidden=true href=#recap>#</a></h2><p>We started with a simple pixel similarity baseline, which is capable of achieving 81% accuracy without any learning.</p><p>Then we moved on to a simple linear model, which we optimized using gradient descent. As we have seen, it is nothing more complicated than a combination of matrix multiplication and derivative calculation.</p><p>Finally, we replaced our linear model with a neural network and saw some accuracy improvements.</p><p>There is something cool in combining a neural net (which is capable of approximating any function giving the right parameters), and gradient descent (which is capable of finding good parameters for any differentiable function) together, isn&rsquo;t it?</p><h2 id=acknowledgements>Acknowledgements<a hidden class=anchor aria-hidden=true href=#acknowledgements>#</a></h2><p>A big thank you goes to <a href=https://felixpatzelt.com>Felix Patzelt</a> for reviewing this post.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://nikita.melkozerov.dev/tags/fast.ai/>fast.ai</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2021 <a href=https://nikita.melkozerov.dev/>Nikita Melkozerov</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script defer src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script><script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position"))},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})});var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft)}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script></body></html>