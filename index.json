[{"content":"Hello folks!\nRecently I trained a handwritten digit classifier using the MNIST dataset from scratch, and it was an eye-opening experience for me. What looked like magic to me before now is just a few mathematical concepts applied together. I was very excited about how simple the whole process was, and I want to share it with everyone who still wonders what kind of magic is happening inside.\nAll you need to know is a bit of python and a few concepts from high-school math. No previous machine learning background is necessary.\nThis post is inspired by chapter 4 of the fast.ai book (I highly recommend it if you\u0026rsquo;re getting started with deep learning), where we tried to build a classifier that can recognize 3\u0026rsquo;s and 7\u0026rsquo;s from the MNIST dataset. This time, however, we will train a classifier to recognize all ten digits of the dataset. I will also try to avoid using \u0026ldquo;magical\u0026rdquo; high-level components as much as possible.\nLet\u0026rsquo;s go!\nWe will use PyTorch and fast.ai libraries for a few useful utilities.\nFirst, we need to import a few functions from the fastai library:\nfrom fastai.vision.all import * from fastbook import * matplotlib.rc(\u0026#39;image\u0026#39;, cmap=\u0026#39;Greys\u0026#39;) Downloading the dataset Let\u0026rsquo;s download the dataset first. Fast.ai has a convenient function to quickly obtain the dataset we\u0026rsquo;re going to use for training:\npath = untar_data(URLs.MNIST) path.ls() (#2) [Path('/home/nm/.fastai/data/mnist_png/training'),Path('/home/nm/.fastai/data/mnist_png/testing')]  The dataset has two folders inside it, training and testing. training should be used for the model training and validation, and testing used to compare the accuracy between different models.\n A small note: I will do a shortcut here and use the testing dataset to validate my model. That is something you shouldn\u0026rsquo;t do in production when you want to evaluate the model\u0026rsquo;s performance. Instead, you should split the training set into training/validation parts. For our toy problem, however, and for the sake of simplicity, I will use the testing set for validation.\n Let\u0026rsquo;s peek inside the training folder:\n(path/\u0026#39;training\u0026#39;).ls() (#10) [ Path('/home/nm/.fastai/data/mnist_png/training/8'), Path('/home/nm/.fastai/data/mnist_png/training/3'), Path('/home/nm/.fastai/data/mnist_png/training/7'), Path('/home/nm/.fastai/data/mnist_png/training/5'), Path('/home/nm/.fastai/data/mnist_png/training/9'), Path('/home/nm/.fastai/data/mnist_png/training/4'), Path('/home/nm/.fastai/data/mnist_png/training/2'), Path('/home/nm/.fastai/data/mnist_png/training/6'), Path('/home/nm/.fastai/data/mnist_png/training/1'), Path('/home/nm/.fastai/data/mnist_png/training/0') ]  What we have here is ten folders, one for each digit. Every folder contains thousands of images of size 28x28 representing some digit.\nYou might say: \u0026ldquo;Well, that\u0026rsquo;s just a bunch of images compressed with a PNG algorithm. How can we do any math operations on them?\u0026rdquo;. You\u0026rsquo;re correct, plain image files are not very useful. To get started, we will first transform the images into a tensor.\n\u0026ldquo;Wait, what is a tensor?\u0026rdquo; you ask, and I\u0026rsquo;d say it a fancy name for a multi-dimensional array. The number of dimensions of this array is also called a rank of a tensor.\nFor example:\nA zero-dimensional array (it is also called scalar) 1 is a tensor of rank 0.\n1-d array aka vector [1,2,3] is a tensor of rank 1.\n2-d arrray aka matrix\n[[1,2,3], [4,5,6], [7,8,9]] is a tensor of rank 2, and so on.\nWe want to convert the images to tensors because then we can perform math operations on them.\nNow we have an image, which is a 2-d array of pixels with pixel intensity values ranging from 0 to 255. Next, we will represent it as a tensor of a rank 2, and scale the pixel values between 0 and 1. Scaling the pixels between 0 and 1 will give us a convenient abstraction for some of the actions we will do next.\nOnce we created a tensor from one image, we will convert other images in our dataset to tensors as well. Then, we will stack the image tensors together into a single tensor of rank 3. I\u0026rsquo;ll explain why we do this in a bit.\nLet\u0026rsquo;s write a function for this:\ndef image_path_to_tensor(path): images_paths = path.ls() tensors = [tensor(Image.open(p)) for p in images_paths] return torch.stack(tensors).float()/255 Having defined the function, let\u0026rsquo;s use it to transform all the images in 10 folders that we have:\nstacked_tensors = [image_path_to_tensor(image_folder) for image_folder in (path/\u0026#39;training\u0026#39;).ls().sorted()] The result is ten tensors that represent the training set of each digit. Let\u0026rsquo;s look at one digit and check the shape of the tensor:\nstacked_tensors[4].shape torch.Size([5842, 28, 28])  The first dimension of the tensor corresponds to an image file, second and third dimensions are the height and the width of the image in pixels.\nBecause we had 5824 images for digit 4, and every image has the size of 28x28 pixels, therefore the resulting shape of the tensor is 5842 by 28 by 28. That\u0026rsquo;s a tensor of rank 3.\nBuilding a baseline: pixel difference Before we start building any machine learning model it is generally a good idea to come up with a simple baseline first. Sometimes the problem at hand can be solved quite well without involving any machine learning at all, and by creating a baseline solution we will be able to justify usage of a machine learning model if the baseline performance is not good enough.\nOne obvious choice would be a random baseline, where each digit is chosen at random, but let\u0026rsquo;s try a little bit harder and build a baseline that relies on pixel similarity.\nThe idea is the following: we will average the image tensors along the first axis, and it will be our \u0026ldquo;pretty average digit\u0026rdquo;. Then we will compare how similar each pixel of the image we try to classify to each pixel of every \u0026ldquo;average\u0026rdquo; digit, and our final guess will be the digit with the smallest error.\nTo start, let\u0026rsquo;s compute our \u0026ldquo;average\u0026rdquo; digit images:\nimage_means = [None] * 10 for digit, imgs_tensor in enumerate(stacked_tensors): mean_tensor = imgs_tensor.mean(0) image_means[digit] = mean_tensor For example, this is how an \u0026ldquo;average\u0026rdquo; 4 looks like:\nNow we need a function that can compute the error. One possible option is to subtract the pixels of two images, square them to make them between 0 and 1, and then take the mean of the result:\ndef calculate_error(image, label): ideal_tensor = image_means[label] return ((image - ideal_tensor)**2).mean() Let\u0026rsquo;s try it out on a digit 3:\nThe average digit of 3 looks like this:\ncalculate_error(stacked_tensors[3][0], 3) tensor(0.0632)  This function is also called mean squared error.\nNow let\u0026rsquo;s create a validation set. As I already mentioned above, using the testing set as a validation set is a shortcut for simplicity, and you should not do the same in production.\nvalid_stacked = [image_path_to_tensor(image_folder) for image_folder in (path/\u0026#39;testing\u0026#39;).ls().sorted()] We\u0026rsquo;re able to compute an error for a single image. Our job now is to compute the error for all images in the validation set. Unfortunately, we can\u0026rsquo;t simply write a loop and do this.\nThe main issue is python loops are unlikely to be vectorized on the GPU. GPU stands for Graphics Processing Unit, and you most likely have one in your computer. GPUs are important for us because they can perform math operations with crazy level of parallelism. But if we will take a single image, put it on the GPU, calculate the error and then remove it from the GPU, it will take us quite a while to calculate the error for the whole validation set.\nInstead of using a loop and processing images one by one, we will grab a tensor with images, put it on the GPU, and then calculate the error for all images in parallel. GPUs are super fast number-crunching machines, and it will take no time for our GPU to do that.\nBut how can we do this? Welcome to broadcasting. Broadcasting is a technique that allows PyTorch to perform operations on tensors with different shapes in a way as they were same-shape tensors. Let me explain it with a simple example.\nIf we have two tensors:\na = [[1,2], [3,4], [5,6]] and\nb = [1,1] then if we write something like c = a + b, we will get the next tensor as a result:\nc = [[2,3], [4,5], [6,7]] Tensor b was added to each element of the tensor a even though they have different ranks. Simply speaking, instead of looping through all elements of the vector a, we told PyTorch to do this in a declarative fashion.\nWith broadcasting in mind, let\u0026rsquo;s define a few functions to measure the distances between our images.\nNa√Øve approach with subtracting one image pixes from another and taking the mean will not work very well. It is possible to have drastically different images where dark and bright pixels will compensate each other, and the mean will be close to zero. To solve this problem, we should make the difference positive and then take the mean.\nFor example, we can calculate a mean absolute error (MAE) and a mean squared error (MSE). The difference between them is that MSE would result in a higher error when images are very different.\ndef mnist_distance_mae(a,b): return (a-b).abs().mean((-1,-2)) def mnist_distance_mse(a,b): return torch.square((a-b)).mean((-1,-2)) The functions above subtract pixels of two images, then take the absolute/squared value and computing the mean. (-1,-2) means that we want to compute the mean along the last and last-1 axis of our tensor, which corresponds to 28x28 image.\n When I was implementing this code, broadcasting led to some very nasty bugs. For example, I had a situation where my model was not able to make any progress in training. Another time the training was much slower than I expected, and I did not know why. As it turned out, there was a bug in the loss function because of implicit broadcasting, and it took me a lot of time to localize the problem. It is very important to check that the shape of the returned tensor on every stage corresponds to what we expect.\n Let\u0026rsquo;s see what is the shape of the result of our distance function:\nmnist_distance_mae(valid_stacked[3], image_means[3]).shape torch.Size([1010])  It is a tensor of size 1010, with one distance per one validation image. That seems to be correct, and we can continue.\nmnist_distance_mse(valid_stacked[3], image_means[3]) tensor([0.0575, 0.0516, 0.0542, ..., 0.0480, 0.0642, 0.0429])  Given the distance functions, now we can write a function which will tell us if our digit prediction is correct:\ndef analyze_digit(digit: str, candidate, distance_function): distances = torch.stack([distance_function(candidate, mean_tensor) for mean_tensor in image_means]) lowest_distance = torch.argmin(distances, dim=0) return lowest_distance == digit analyze_digit(4, stacked_tensors[3][113], mnist_distance_mae) tensor(False)  Let\u0026rsquo;s put broadcasting into work and do analyze all 3s at once:\nanalyze_digit(3, valid_stacked[3], mnist_distance_mae).float().mean() tensor(0.6089)  Our accuracy is about 60%. It is better than random (it would be around 10), but we can do better. What is the mean accuracy for all digits that we have? We can use both of our distance functions and compare the results:\naccuracy_mae = torch.stack([analyze_digit(digit, valid_stacked[digit], mnist_distance_mae).float().mean() for digit in range(10)]) accuracy_mae tensor([0.8153, 0.9982, 0.4234, 0.6089, 0.6680, 0.3262, 0.7871, 0.7646, 0.4425, 0.7760])  accuracy_mse = torch.stack([analyze_digit(digit, valid_stacked[digit], mnist_distance_mse).float().mean() for digit in range(10)]) accuracy_mse tensor([0.8959, 0.9621, 0.7568, 0.8059, 0.8259, 0.6861, 0.8633, 0.8327, 0.7372, 0.8067])  As we see, MSE gives us better results since it penalizes bigger differences more compared to smaller ones. Let\u0026rsquo;s look at the mean accuracies:\naccuracy_mae.mean() tensor(0.6610)  accuracy_mse.mean() tensor(0.8173)  One observation is that MSE gives us better accuracy compared to MAE.\nWe got 82% accuracy without any learning at all! We can take it as our baseline, and our goal would be to train a machine learning model which can do this better. It is always a good idea to start with a simple solution and then move to a more sophisticated one if the observed performance is below expectations.\nMachile Learning time! Now, we\u0026rsquo;re finally going to do some machine learning.\nDatasets First, we will concatenate our images for different digits into a single tensor, and then we will represent every 28x28 image as 1x784 vector. I\u0026rsquo;ll explain why we do it in a bit. This tensor is going to be the \u0026ldquo;input\u0026rdquo; for our model.\ntrain_x = torch.cat([stacked_tensors[i] for i in range(10)]).view(-1, 28*28) Next, let\u0026rsquo;s create another tensor containing the correct labels for every image in our previous tensor.\ntrain_y = torch.cat([torch.stack([tensor(i)]*len(stacked_tensors[i])) for i in range(10)]) train_x.shape,train_y.shape (torch.Size([60000, 784]), torch.Size([60000]))  Finally, we will combine them in a dataset. A dataset is just a simple pair of inputs and outputs to the model, in our case, it is the images (train_x) and the labels (train_y).\ndataset = list(zip(train_x,train_y)) We can also look at what is inside. To do this, we want to convert 1x784 tensor into a 28x28 one. Method view of a tensor can help us with it:\nshow_image(dataset[32000][0].view(-1,28,28)) And the label is also 5:\ndataset[32000][1] tensor(5)  Now we perform the same operations for the validation set:\nvalid_x = torch.cat([valid_stacked[i] for i in range(10)]).view(-1, 28*28) valid_y = torch.cat([torch.stack([tensor(i)]*len(valid_stacked[i])) for i in range(10)]) valid_x.shape,valid_y.shape (torch.Size([10000, 784]), torch.Size([10000]))  valid_dset = list(zip(valid_x,valid_y)) Linear model Before we continue, let\u0026rsquo;s quickly talk about what we have done and what we\u0026rsquo;re going to do next.\nWe have already collected the training and the validation datasets, and the dataset is just a collection of images and the corresponding labels. We also transformed every image into a vector.\nWhat we need now is to define a function, which will take an image as an input and will produce the prediction as an output.\nTo be more specific, our function will accept a vector of size 784 as an input, where each element of this vector will correspond to a pixel of an image. Since our function does not care about the arrangement of input values, we reshaped our tensor earlier to make it more convenient for us to work with it. The produced result will be a vector of size 10, where each number will represent the probability of each digit.\nIn the code, it would be something like this:\ndef predict(pixels: List[float]): List[float] But what kind of function is capable of doing such a transformation? To our luck, such functions exist, and they are called neural networks!\nIt is proven that given enough parameters, a neural network with only one hidden layer is capable of approximating any function with an arbitrary level of precision. How cool is that! For more information, look for the universal approximation theorem.\nWe, however, will start with a degenerate case of a neural network called a linear function.\nIn pseudocode it looks like this:\ndef linear(x: float, weight: float, bias: float) -\u0026gt; float: return x * weight + bias This function accepts an input, number x, a parameter called weight, and a parameter called bias. Then the input is multiplied by the weight and the bias added in the end. Sounds simple, right?\nHowever, in our case, we have not a single but 784 input parameters. Let\u0026rsquo;s for now pretend that we\u0026rsquo;re only interested in predicting a single digit:\nfrom typing import List def linear(x: List[float], weights: List[float], bias: float) -\u0026gt; float: return (x * weights).sum() + bias Here * stand for element-wise vector multiplication, meaning that if we have two vectors,\na = [1, 2, 3] b = [1, 2, 3] then a * b would be the vector [1*1, 2*2, 3*3]\nThe result of this function will be a likeliness that our input x corresponds to a label. But what if we want to predict more than one label?\nThat\u0026rsquo;s simple as well. First, we define a few sets of weights and biases, one per class we want to predict. Then we call the function several times to make a prediction for each class. The number returned by the functions represents neural net confidence in the predicted digit. Then we interpret the result with the highest number as the predicted class. Let\u0026rsquo;s say we only want to predict three digits, then what we want to do is next:\nscore_0 = linear(image, weights_0, bias_0) score_1 = linear(image, weights_1, bias_1) score_2 = linear(image, weights_2, bias_2) Finally, we check which resulted in the highest score, and this is going to be the predicted class.\nLet\u0026rsquo;s wrap it in a function:\ndef predict(image: List[float], weights: List[List[float]], biases: List[float]) -\u0026gt; List[float]: results = [] for digit in range(3): likeliness = linear(image, weights[digit], biases[digit]) results.append(likeliness) return results That\u0026rsquo;s it! Our simple linear model will be able to predict an image from pixels.\nHowever, we have two problems:\n This function is going to be extremely slow since GPUs do not like loops, and the loop we defined earlier will be executed using plain python runtime. We don\u0026rsquo;t know where to get these magical weights and biases!  To deal with the first problem, we will utilize PyTorch tensors, which will give us free GPU parallelization.\nAs for the second problem, we need to learn about Stochastic Gradient Descent (SGD).\nEverything is a Tensor Since we want to use GPU for computation, we need to represent our weights and biases as tensors. Let\u0026rsquo;s write a helper function for this.\ndef init_params(size): return torch.randn(size).requires_grad_() Method .requires_grad_() tells PyTorch to track the operations done with the tensor so that later we can ask PyTorch to compute gradients for us. More on that in a bit.\nRemember the weights in the predict function? We wanted to have something like this List[List[float; 784]; 10] (The number next to the type is the size of the list). That is nothing else than a matrix of size 784 by 10.\nIf you\u0026rsquo;re wondering where we got these magic numbers, it comes from the following. The number 784 used because our input image has a size of 28x28, and later we unroll it into a 1x784 vector. Number 10 comes from the ten classes of digits we\u0026rsquo;re going to recognize.\nLet\u0026rsquo;s initialize the weights randomly:\nweights = init_params((28*28,10)) The same goes for biases:\nbiases = init_params(10) Now we can try to calculate the predictions for a single image. Instead of looping for every digit and multiplying vectors, we can do everything in one single step using matrix-vector multiplication. And PyTorch will use the power of the GPU to do this operation as quickly as possible!\ntrain_x[34000] @ weights + biases tensor([ 5.4221, -1.5908, 17.0564, -4.3119, -15.0097, -8.3001, 0.5106, 2.3884, -6.6653, 13.2021], grad_fn=\u0026lt;AddBackward0\u0026gt;)  The new operator @ above is a PyTorch operator for matrix multiplication. I will not get into details about what matrix multiplication is, but a nice and intuitive explanation of it can be found on http://matrixmultiplication.xyz/.\nLet\u0026rsquo;s write a function that will represent our linear model\ndef linear_model(x_batch): return x_batch @ weights + biases predictions = linear_model(train_x[34000]) predictions tensor([ 5.4221, -1.5908, 17.0564, -4.3119, -15.0097, -8.3001, 0.5106, 2.3884, -6.6653, 13.2021], grad_fn=\u0026lt;AddBackward0\u0026gt;)  One interesting property of our matrix-powered function is that this function can process a batch of several images at once. Instead of providing a single vector as an input, we can provide a matrix containing a few images, then do the matrix multiplication, and then we will get the matrix with the predictions back.\nWith this in mind, we can calculate predictions for the whole dataset without writing a single loop:\npredictions = linear_model(train_x) predictions.shape torch.Size([60000, 10])  argmax will return the index of the tensor with the biggest value. Since we expect the model to return bigger numbers for the predictions it is more confident with, the number returned will be the predicted class. Let\u0026rsquo;s calculate the accuracy:\n(predictions.argmax(dim=1) == train_y).float().mean().item() 0.12043333053588867  The model initialized using random parameters made around 10% of predictions correct. That\u0026rsquo;s pretty much what we would expect for a random model since the probability of being correct with a random guess is 0.1.\nSGD So we solved the first problem, the prediction function is fast and can be run on a GPU. But we still have no idea where to get the proper (non-random) parameters.\nNow, let\u0026rsquo;s talk about gradient descent (the \u0026ldquo;stochastic\u0026rdquo; part will be explained later).\nAs you might remember from a calculus course, a derivative of a function is another function that represents a rate of change of a function output with respect to its input. If we evaluate a derivative at some point where our function is defined, we can think of a result as a direction in which the function evaluated at that point will grow the fastest. If we know this, we can easily figure out in which direction we need to go in order to reach the minimum of the function.\nIn case our function operates on several input variables, instead of the derivative we will calculate a gradient, which can be imagined as an \u0026ldquo;advanced\u0026rdquo; version of the derivative capable of working with multi-variable functions. The gradient is similar to the derivative since it will give us the direction in which the function is going to grow the fastest.\nOnce we have the gradient, we can multiply it by -1 to get the direction in which the function will decline the fastest, then multiply the gradient by a small value and update the parameters. After we do this, the function we\u0026rsquo;re interested in will produce a smaller value given the updated parameters.\nUsing this information, we can describe the gradient descent process as following:\ninitialize function parameter randomly while (function parameters are not good enough): calculate gradient parameters = parameters - gradient * step The gradient descent is capable of optimizing any differentiable function, and we will utilize this for our training process.\nNext, we need to define a so-called loss function, which will tell us how good or bad the performance of our linear model is when we change the parameters.\nOnce we define it, we will run the gradient descent method with the loss function and find the optimal parameters.\nLoss Function Let\u0026rsquo;s think about what we want from our loss function. We want it to take images, parameters, and labels as an input and produce a number that shows how \u0026ldquo;bad\u0026rdquo; our linear model is.\nTaking this into account, our first implementation of the loss function will look like this:\ndef mnist_loss(predictions, targets, nr_classes=10): predictions = predictions.sigmoid() mask = F.one_hot(targets, nr_classes) errors = ((mask-predictions) ** 2) return errors.mean() Let\u0026rsquo;s analyze it line by line:\npredictions = predictions.sigmoid() We apply a sigmoid function to each prediction.\ndef sigmoid(x): return 1/(1+torch.exp(-x)) The sigmoid function maps its input to a range between 0 and 1, and it looks like this:\nOur idea is to subtract correct predictions from 1-s, so that when our model is correct and confident in the prediction, the error will be close to zero. With other predictions, we will not do anything, which will result in a low error of the model does not give us high confidence for incorrect labels.\nmask = F.one_hot(targets, nr_classes) Here we generate a mask, which will represent the correct labels. For example F.one_hot(tensor([0,2,1]), 3) will give us\n[[1,0,0], [0,0,1], [0,1,0]] Next, we will subtract our prediction from the mask. To deal with negative numbers we will take a squared value of the error, which will keep the function differentiable and additionally will have a nice bonus of punishing big errors more than small ones.\nerrors = ((mask-predictions) ** 2) In the end, we will take a mean to reduce the error tensor to a single number:\nerrors.mean() Having defined a lost function, we want to calculate its gradient. However, we run into a problem: our loss function operates on predictions and labels and not on the model parameters.\nTo include the weights and biases into the gradient calculating, we will take a derivative of a new function which is a composition of the model and the loss function. The new function will look like this:\ndef composed(x_batch, y_batch): preds = linear_model(x_batch) loss = mnist_loss(preds, y_batch) Now we need to calculate a gradient. If we do that by hand via calculating partial derivatives using a chain rule, the process would be a bit tedious because this function operates on thousands of parameters.\nFortunately for us, PyTorch can solve this quickly. What we need to do is to call .requires_grad() on a tensor in which gradients we\u0026rsquo;re interested before computing the function, and then call .backward() to tell PyTorch that we want the gradients.\nHere is how we do it:\ndef calc_grad(x_batch, y_batch, model, loss_fn): preds = model(x_batch) loss = loss_fn(preds, y_batch) loss.backward() Let\u0026rsquo;s talk about how we will feed the data to our gradient descent process. One option is to feed it one image at a time, but this will take an unreasonable amount of time to complete due to data moving overhead. Training on the whole dataset at once is not desirable either, since the dataset might be simply too big to fit on the GPU. What is commonly done is the following: the dataset is divided in batches, and then we calculate gradients using the whole batch. That\u0026rsquo;s where the word stochastic comes from, mainly because batched contain shuffled items, and the gradient descent is run on data randomly sampled from the dataset.\nWhen we iterate through the dataset, we generally prefer to have diverse examples in our batches because this leads to better generalization. An easy way to achieve this is to shuffle the dataset. Fortunately for us, PyTorch provides a class called DataLoader, which does the shuffling and batch separation for us:\ndl = DataLoader(dataset, batch_size=256,shuffle=True) valid_dl = DataLoader(valid_dset, batch_size=256, shuffle=False) Training loop We can also write a function that runs SGD using the whole dataset (an iteration through all images called an epoch):\ndef train_epoch(model, loss_fn, lr, params): for xb,yb in dl: calc_grad(xb, yb, model, loss_fn) for p in params: p.data -= p.grad * lr p.grad.zero_() For every batch, we compute the gradients, multiply them by a small number called the learning rate, and subtract the result from our initial parameters. The learning rate should be low enough to keep the process stable and at the same time large enough so that our training does not last forever.\nIt is important to know how accurate our model is. We will start with the accuracy of a single batch:\ndef batch_accuracy(xb, yb): correct = (xb.argmax(axis=1) == yb).float().mean() return correct Once we know how to measure the accuracy of the batch, we can measure the accuracy of the model in a single epoch:\ndef validate_epoch(model): accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl] return round(torch.stack(accs).mean().item(), 4) Let\u0026rsquo;s see what the accuracy of our randomly-initialized model is:\nvalidate_epoch(linear_model) 0.1272  Something around 10% is what we expect.\nFinally, we can train a single epoch:\naccuracy = [] lr = 1. # initializing random weights weights = init_params((28*28,10)) # initializing random biases biases = init_params(10) params = weights,biases # training an epoch train_epoch(linear_model, mnist_loss, lr, params) # validating results accuracy.append(validate_epoch(linear_model)) accuracy[0] 0.1405  Let\u0026rsquo;s train for 40 epochs:\nfor i in range(40): train_epoch(linear_model, mnist_loss, lr, params) accuracy.append(validate_epoch(linear_model)) plt.figure(figsize=(10,6)) plt.plot(accuracy) plt.ylabel(\u0026#39;accuracy\u0026#39;) plt.xlabel(\u0026#39;epoch\u0026#39;) plt.show() Note that the epoch index starts at 0.\nThe training starts fast, but it slows down as model accuracy increases. The problem is our model can be very certain about several different classes for the same image. However, this is not the task we\u0026rsquo;re trying to solve. The reason for this behavior lies in our loss function. Remember, the loss function first applies sigmoid to keep the function output between 0 and 1. This only scales the output, but what we want is that the model will select one label at the end.\nLet\u0026rsquo;s replace the sigmoid function with a so-called softmax function. It is similar to sigmoid when we\u0026rsquo;re predicting a single class, but it will make the output probabilities sum to 1 if we have more than one class. Thus, only the relative difference of the predictions will be important, and a high degree of confidence in one class will automatically decrease confidence in other classes.\ndef mnist_loss_softmax(predictions, targets, nr_classes=10): predictions = torch.softmax(predictions, axis=1) mask = F.one_hot(targets, nr_classes) errors = ((mask-predictions) ** 2) return errors.mean() Let\u0026rsquo;s train the model again:\naccuracy = [] lr = 1. # initializing random weights weights = init_params((28*28,10)) # initializing random biases biases = init_params(10) params = weights,biases # training an epoch train_epoch(linear_model, mnist_loss_softmax, lr, params) # validating results accuracy.append(validate_epoch(linear_model)) accuracy[0] 0.1619  for i in range(40): train_epoch(linear_model, mnist_loss_softmax, lr, params) That helped, but our learning is still getting slower as the accuracy is getting higher, and we have not yet surpassed our baseline.\nLet\u0026rsquo;s blame the loss function again! Now we can think of a problem we face as following: for our loss function the absolute difference between parameters that give 0.9 and 0.99 accuracies is small, about 0.1. But if we think about it, the second set would give us 10x more accurate results! To address this problem, we\u0026rsquo;re going to apply a negative logarithm fucntion to the results. It will rescale our outputs in a way so the SGD will find the right direction easier.\nThe negative logarithm function looks like this:\nSuch a loss function is then called a cross-entropy loss:\ndef cross_entropy_loss(preds, y): # apply softmax preds = torch.softmax(preds, axis=1) # get confidences for the correct class idx = len(preds) confidences = preds[range(idx), y] # calculate negative log likelihood and return its mean log_ll = -torch.log(confidences) return log_ll.mean() Let\u0026rsquo;s train the model again using the new shiny loss function:\naccuracy = [] lr = 1. # initializing random weights weights = init_params((28*28,10)) # initializing random biases biases = init_params(10) params = weights,biases # training an epoch train_epoch(linear_model, cross_entropy_loss, lr, params) # validating results accuracy.append(validate_epoch(linear_model)) accuracy[0] 0.8431  Wow, already after training for a single epoch we got 85% accuracy, outperforming our baseline. That\u0026rsquo;s a huge improvement! Let\u0026rsquo;s train for a few more epochs:\nfor i in range(40): train_epoch(linear_model, cross_entropy_loss, lr, params) 91.5% accuracy with a simple linear model! Isn\u0026rsquo;t it impressive?\nBut so far out model can hardly be called a neural network since it consists of a single linear layer. To make it a \u0026ldquo;real\u0026rdquo; neural net, we should add some-non-linearity. For example, we can add a sigmoid layer in-between two linear layers and then use function composition to apply them sequentially:\nw1 = init_params((28*28,64)) b1 = init_params(64) w2 = init_params((64,10)) b2 = init_params(10) def simple_neural_net(xb): res = xb@w1 + b1 # 1st linear layer res = torch.sigmoid(res) # 2nd non-linear layer res = res@w2 + b2 # 3rd linear layer return res params = [w1,b1,w2,b2] accuracy = [] lr = 1. # training an epoch train_epoch(simple_neural_net, cross_entropy_loss, lr, params) # validating results validate_epoch(simple_neural_net) 0.8372  for i in range(40): train_epoch(simple_neural_net, cross_entropy_loss, lr, params) Around 95% accuracy! That\u0026rsquo;s definitely better compared to the plain linear model.\nAs you can see, our new non-linear model is a drop-in replacement for the old one (as long as it has the same number of inputs and outputs), and the rest of the process stays exactly the same.\nFinally, as a fun example let\u0026rsquo;s see how this task can be done using a neural network designed for image recognition:\ndls = ImageDataLoaders.from_folder(path, train=\u0026#34;training\u0026#34;, valid=\u0026#34;testing\u0026#34;) learn = cnn_learner(dls, resnet18, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(4, 0.1)    epoch train_loss valid_loss accuracy time     0 1.519870 1.372195 0.914800 00:33   1 0.698865 53.716496 0.968600 00:32   2 0.167927 0.049071 0.988100 00:33   3 0.031542 0.020860 0.994100 00:34    Wow, 99.4% only after four epochs! Those modern neural networks are powerful.\nIt is worth mentioning that SGD is not ideal. For example, there are no guarantees that it actually finds good parameters in a finite number of steps, let alone optimal ones.\nAlso, a neural network is a non-convex function, so there may be many local minimums where the SGD can get stuck. This is why simpler methods are often better suited to problems that can be solved with the indicated simpler methods.\nFinally, ML researchers have made significant progress in the area of SGD, and neural networks and modern SGD + regularization have achieved state-of-the-art results for many problems.\nRecap We started with a simple pixel similarity baseline, which is capable of achieving 81% accuracy without any learning.\nThen we moved on to a simple linear model, which we optimized using gradient descent. As we have seen, it is nothing more complicated than a combination of matrix multiplication and derivative calculation.\nFinally, we replaced our linear model with a neural network and saw some accuracy improvements.\nThere is something cool in combining a neural net (which is capable of approximating any function giving the right parameters), and gradient descent (which is capable of finding good parameters for any differentiable function) together, isn\u0026rsquo;t it?\nAcknowledgements A big thank you goes to Felix Patzelt for reviewing this post.\n","permalink":"https://nikita.melkozerov.dev/posts/2021/02/image-classification-from-scratch-without-a-phd/","summary":"Hello folks!\nRecently I trained a handwritten digit classifier using the MNIST dataset from scratch, and it was an eye-opening experience for me. What looked like magic to me before now is just a few mathematical concepts applied together. I was very excited about how simple the whole process was, and I want to share it with everyone who still wonders what kind of magic is happening inside.\nAll you need to know is a bit of python and a few concepts from high-school math.","title":"Image Classification from scratch without a PhD"},{"content":"TL;DR: batch size 32 is probably going to be a good default candidate for many cases.\nIn this post, we will observe how different batch sizes change learning metrics when we train a model using Transfer Learning and the fast.ai library. We will try to find out which batch sizes are good and which are better to be avoided.\nGetting the dataset Recently I was going through the awesome fast.ai deep learning course, and in one of the lectures we were building a classifier that can recognize cats and dogs. I wanted to build one too, and since I live in Hamburg I decided to go with some birds one can see there. Fortunately, there was a post by Luca Feuerriegel where I found the names of some of the species: European Robin, Marsh Tit, Eurasian Blackbird, Eurasian Nuthatch, Eurasian Jay, Eurasian Wren, Hawfinch, Bullfinch, Common Starling, Greylag Goose, Barnacle Goose, Meadow Pipit, Common Wood Pigeon, Mistle Thrush.\nTo collect the dataset, I used the bing image search to get images of every bird mentioned above. I\u0026rsquo;m not going to publish the collected images since I have doubts about violating copyright, however, you can download the source code of the notebook and run the experiment yourself.\nAfter I collected the dataset, I did a few training iterations and cleaned the dataset using the ImageClassifierCleaner tool in the fast.ai library.\nIn the end, I ended up with 2017 pictures representing 14 different species of birds, which should be good enough to train the model fast and not overfit too quickly.\nTraining the model To obtain the results we\u0026rsquo;re going to experiment with 3 ResNet architectures: ResNet50, ResNet34, and ResNet18. For each architecture, we will train the model 10 times with batch sizes of 128, 64, 32, 16, 8, and 4. We will also train the model for 10 epochs for each combination of the architecture and batch size.\nWe\u0026rsquo;re also going to apply a few transformations and data augmentation steps to avoid overfitting: randomly cropping and resizing the images, and applying a standard set of batch augmentation (aug_transforms()):\n1 2 3 4 5 6 7  birdsDB = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=1337), get_y=parent_label, item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms())   The other parameters will be left to default. We will also use a CSVLogger callback to save the learning metrics to the CSV files which we will analyze later.\n1 2 3 4  def train(arch, batch_size, index): dls = birdsDB.dataloaders(path, bs=batch_size) learn = cnn_learner(dls, architectures[arch], metrics=error_rate, cbs=[ShowGraphCallback, CSVLogger(fname=f\u0026#39;birds-{arch}-bs{batch_size}-{index}.csv\u0026#39;)]).to_fp16() learn.fine_tune(10)   We will also train with half-precision to fit ResNet50 with 128 batch size into my GPU.\nFinally this is our nested loop where we will try different parameters:\n1 2 3 4 5 6  for arch in [\u0026#39;rn50\u0026#39;, \u0026#39;rn34\u0026#39;, \u0026#39;rn18\u0026#39;]: for bs in [128, 64, 32, 16, 8, 4]: for index in range(10): train(arch, bs, index) torch.cuda.empty_cache() gc.collect()   It is worth noting that this loop took around 6 hours to finish, so be patient if you would like to experiment yourself :)\nAnalyzing the results After waiting for a few hours, we finally have all 180 CSV files ready for analysis. Yay!\nLet\u0026rsquo;s dig into them.\nFirst we would need a function to parse the CSV file and convert it into a pandas dataframe:\n1 2 3 4 5 6 7  def loadLog(bs: int, arch: str, idx: int) -\u0026gt; pd.DataFrame: df = pd.read_csv(f\u0026#34;birds-{arch}-bs{bs}-{idx}.csv\u0026#34;) # change the time to seconds # since no training epoch took more than 1 minute  # we will cheat and simply trim the minutes away df[\u0026#39;time\u0026#39;] = df.apply(lambda df: int(df[\u0026#39;time\u0026#39;].split(\u0026#39;:\u0026#39;)[1]), axis=1) return df   Then, because we have 10 dataframes per each combination of a batch size and an architecture, we will merge them together and calculate the average:\n1 2 3 4  def loadMergedLog(bs: int, arch: str) -\u0026gt; pd.DataFrame: dfs = map(lambda idx: loadLog(bs, arch, idx), range(10)) merged = pd.concat(dfs) return merged.groupby(merged.index).mean()   Finally, we need a function that will plot the results:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  def plotResults(arch: str, y_axis: str): fig, ax = plt.subplots() fig.set_size_inches(20,15) blockSizes = [128, 64, 32, 16, 8, 4] for bs in blockSizes: frame = loadMergedLog(bs, arch) plt.plot(frame[\u0026#39;epoch\u0026#39;], frame[y_axis]) ax.legend(blockSizes); ax.legend(blockSizes); plt.xlim(0,9) plt.show()   With all this in place, let\u0026rsquo;s see how the batch size was affecting the training:\nResNet50 Let\u0026rsquo;s start with the error rate:\nHere we see that batch sizes 4 and 8 are not that good, and 32 gave us the lowest error after 10 epochs of training.\nNow let\u0026rsquo;s look at the training time:\nUnsurprisingly, batch sizes 4 and 8 were slow due to copying overhead, while batch sizes of 32 and 64 were the fastest. Interestingly, a batch size of 128 was also slower than 32 and 64.\nResNet34 With a reduced number of layers the model error rate seems to follow the same pattern as before: batch size 32 looking better than the others (however not that much) and batch size 4 again showed the lowest performance.\nSpeaking of training time we see the same picture: batch sizes of 32 and 64 being the fastest, and 4 being the slowest.\nInitially, the learning performance doubles when we double the batch size (bs16 is twice as fast as bs8, and bs8 is twice as fast as bs4), and stabilizes around 32 and 62 images per batch.\nResNet18 When we train the model using an even smaller ResNet architecture, our previous results are confirmed again:\nLearning was the fastest with batch size 32, and the performance of all three 16, 32, and 64 batch sizes are very similar.\nResults We trained the classifier on the natural images resized to 224 pixels, and discovered that batch size 32 was often surpassing other candidates in terms of learning speed and error rate.\nThis means that it is probably going to be a good default candidate when we try to analyze natural images and want to iterate quickly, for example when wanting to clean up the dataset.\nBatch sizes of 8 and less are probably better be avoided if your images are small due to high overhead on data transfer.\nTraining with a batch size of 128 was slower and a bit less accurate, so it might not be the ideal candidate to start with.\n","permalink":"https://nikita.melkozerov.dev/posts/2021/01/transfer-learning-and-resnet-in-search-of-a-perfect-batch-size/","summary":"\u003cp\u003eTL;DR: \u003cem\u003ebatch size 32 is probably going to be a good default candidate for many cases.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eIn this post, we will observe how different batch sizes change learning metrics when we train a model using Transfer Learning and the fast.ai library. We will try to find out which batch sizes are good and which are better to be avoided.\u003c/p\u003e","title":"Transfer Learning and ResNet: In search of a perfect batch size"},{"content":"How to prepare your FoundationDB cluster to run it in production and set up process classes to achieve maximum performance.\nFoundationDB is an outstanding database and it is a joy to use it as a developer. However, when it comes to running FDB in production, it is not always clear how many instances are needed and what classes should be assigned to processes. As of June 2019, FoundationDB doesn\u0026rsquo;t (yet) have a \u0026ldquo;smart\u0026rdquo; tool that can automatically set the layout of the FDB cluster, and it is important to set the layout manually to achieve decent performance.\nIn this post, I\u0026rsquo;ll writte about the best practices for configuring FDB cluster which I discovered while experimenting with the database and reading FoundationDB forums.\nRoles, Classes, and Processes in FoundationDB FoundationDB server processes are identical binaries, but it is possible to assign a process to a specific class or a role to prioritize a specific workload. The difference between a class and a role is that assigning a class to a process specifies preference to recruit one of the corresponding roles (based on how good a particular role fits to a class), and assigning a role will result in a process preferring a single workload. The relationships between classes and roles are defined in the Locality.cpp file.\nOut of clutter, find simplicity While no one prevents assigning a single role to a process, it is more flexible to assign storage, transaction and stateless classes to processes. Let\u0026rsquo;s look at them more closely:\nStorage class: best fit for the storage role, worst fit for the transaction log role. Other roles won\u0026rsquo;t be recruited in a process with this class.\nTransaction class: good fit for the transaction log role, okay fit for the proxy, resolver, log router, cluster controller roles, worst fit for the storage role.\nStateless class: good fit for the proxy, master, resolver, log router, cluster controller, data distributor, and ratekeeper processes. Storage and transaction log roles won\u0026rsquo;t be assigned to the stateless process.\nWhen FDB tries to figure out which role it will recruit in a process, the fitness priority is used: best fit \u0026gt; good fit \u0026gt; okay fit \u0026gt; unset fit \u0026gt; worst fit \u0026gt; never assign. For example, if there are two processes which both have a transaction and a stateless class, the proxy role will be assigned to the stateless class since it fits better, even though transaction class is also suitable for this role.\nPlease note that assigning a role to the process will be treated as a class with the best fit for that role.\nEvery individual has a role to play for the betterment of our cluster So, if a process will be recruited for a particular role, what will it do? Let\u0026rsquo;s look at them one by one:\n Process with the storage role is responsible for storing key/value b-trees and serving reads to clients. It provides a consistent database snapshot for the last 5 seconds. A typical FDB cluster would have ~ 80-90% of all processes recruited as storage. The transaction log process keeps an append-only log of mutations for ~ 7 seconds before the changes are pulled by the storage process, then deletes them. If a storage process is not able to pull the changes in time, the disk space used by log will grow. The proxy sits between the client and transactional authority and coordinates the actions along the write transaction path. It also gives out recent read versions to the client. The master keeps track of the commit version, assigns commit versions to transactions to provide global ordering. The resolver checks transactions and aborts them if they\u0026rsquo;re conflicting. The data distributor is responsible for distributing data across shards. The ratekeeper limits the transaction rate to prevent cluster overload. The cluster controller knows the cluster configuration, provides it to new clients and reconfigures the cluster in the event of network disruption. The log router will be recruited in remote datacenters for the purpose of supporting the log replication.  Now that we know more about the classes, roles, and processes, let\u0026rsquo;s see what resource requirements different roles have.\nRequirements and resources Different process roles consume different resources such as CPU, IOPS, network bandwidth and RAM, and we need to be careful to not create resource contention that will lead to reduced performance.\nThe transaction Log is both IOPS1 and bandwidth2 hungry, and it\u0026rsquo;s important to dedicate a whole disk for it. Memory consumption is ~ 1GB3.\nThe storage is not IOPS hungry but it can saturate the CPU, so it\u0026rsquo;s better to not run the storage role in one process with others. If your SSD is fast enough, having more than one storage processes will utilize it better. It consumes less bandwidth than tLog or proxy, but it usually requires ~ 4GB of memory on average3.\nThe proxy is bandwidth2 \u0026amp; CPU hungry, and it is important to give this role enough CPU to keep the latencies low.\nThe resolver is involved in checking conflict ranges and if you don\u0026rsquo;t have conflicts then CPU consumption will be low4. No high bandwidth or high IOPS consumption is expected, but the memory consumption can around 1GB3.\nThe rest of the stateless processes do not consume any significant resources and can be deployed without restrictions.\nBased on the requirements above, there is a set of recommendations:\n Don\u0026rsquo;t run tLog and storage on a single disk. Don\u0026rsquo;t run anything else in a single process that has storage or proxy roles since they\u0026rsquo;re CPU hungry. Run each proxy on a different host. It might be useful to dedicate a separate host for tLog due to its bandwidth consumption. Don\u0026rsquo;t leave unspecified processes as FDB will recruit a storage role there by default5, and this can increase latency for other roles in this process.  These are not hard rules but rather recommendations that should improve the performance of your cluster. For example, if a proxy is consuming the whole CPU core but the network is not saturated, it makes sense to run another proxy process on the same host.\nBuilding the cluster Given that we understand more about roles and their requirements, let\u0026rsquo;s see what might be reasonable cluster layouts.\n2-node replication If we want to tolerate a single machine failure (and make progress after such failure), we need 3 machines with 2 copy replication. However, for performance reasons, we should not share disks between tLog and storage roles. It means that we should have 6 separate disks. Often, cloud providers will give only one disk per node, that\u0026rsquo;s why we need at least six different nodes to prevent performance penalties in the event of failure.\nA minimal Layout #1: 6 nodes, 2 processes in each node:\n tLog master tLog cluster_controller tLog stateless storage proxy storage proxy storage stateless  In this layout, we try to utilize the network bandwidth as much as possible by placing tLogs and proxies on different nodes, and at the same time, we don\u0026rsquo;t have a performance loss in case of failure because we have extra tLog and storage processes. You can easily scale this cluster by adding more storage processes, and adding more tLog, proxies, and resolver if they\u0026rsquo;re overloaded.\nA minimal Layout #2: 5 nodes, 2 processes in each node:\n tLog master tLog cluster_controller storage proxy storage proxy storage stateless  Here we use fewer nodes, however, if tLog fails FDB will relocate the missing tLog to the same process together with a storage role. This will degrade the performance and increase latencies until the node is back.\nPlease note that the ratio of tLog : storage is not optimal here and it\u0026rsquo;s done this way purely due to fault tolerance concerns. Usually, you need more storage processes than tLog processes. It depends on your workload though, if you have lots of heavy writes, then having more tLog processes is better.\nAlso, don\u0026rsquo;t forget to configure three coordinators to keep the cluster running in the event of a node failure.\n3-node replication If you\u0026rsquo;re unsure about the reliability of the available hosts, it is possible to use 3-node replication. Again, I\u0026rsquo;m making this layout given that only 1 disk is available per host, therefore we need to have 4 hosts for each tLog and storage role.\nA minimal Layout #3: 8 nodes, 2 processes in each node:\n tLog master tLog cluster_controller tLog stateless tLog stateless storage proxy storage proxy storage proxy storage resolution  With this layout we don\u0026rsquo;t share a node between the proxy and the tLog, disks aren\u0026rsquo;t shared between stateful processes, and every process has a set class so FDB won\u0026rsquo;t spawn a storage process where we wouldn\u0026rsquo;t expect it. We already have quite some tLog roles here, so to scale this cluster we should add more storage nodes.\nThat\u0026rsquo;s all, folks. I hope these layouts will make it easier to bootstrap a FoundationDB cluster. Do you have other cool ideas about the possible cluster layouts? Please share them in the comments!\n  https://forums.foundationdb.org/t/production-deployment/522/4?u=meln1k \u0026#x21a9;\u0026#xfe0e;\n https://forums.foundationdb.org/t/production-optimizations/601/11?u=meln1k \u0026#x21a9;\u0026#xfe0e;\n https://forums.foundationdb.org/t/constrained-ram-in-an-application-development-environment/347/4?u=meln1k \u0026#x21a9;\u0026#xfe0e;\n https://forums.foundationdb.org/t/configuring-foundationdb-to-use-more-than-one-resolver/1414/2?u=meln1k \u0026#x21a9;\u0026#xfe0e;\n https://forums.foundationdb.org/t/production-optimizations/601/13?u=meln1k \u0026#x21a9;\u0026#xfe0e;\n  ","permalink":"https://nikita.melkozerov.dev/posts/2019/06/building-a-foundationdb-cluster-roles-classes-and-processes/","summary":"\u003cp\u003eHow to prepare your FoundationDB cluster to run it in production and set up process\nclasses to achieve maximum performance.\u003c/p\u003e","title":"Building a FoundationDB Cluster: Roles, Classes, and Processes"}]